"","phase","step","Name","Comment","response"
"1",1,0,"1-problem_formulation","Problem Formulation - specifies the decision-making context in which the model will be used, the clients driving model development or stakeholders addressed by the model. It also includes specification of model outputs, statements of the domain of applicability of the model, as well as the extent by which model outputs may be acceptably extrapolated (Grimm et al. 2014).",""
"2",1,1,"1.1-problem_formulation: define model purpose","Defining the purpose of the model is critical because the model purpose influences choices at later stages of model development [@Jakeman:2006ii]. Common model purposes in ecology include: gaining a better qualitative understanding of the target system, knowledge synthesis and review, and providing guidance for management and decision-making [@Jakeman:2006ii] (could also see Addison paper on the use of models). Note that modelling objectives are distinct from the analytical objectives of the model.    ",""
"3",1,1,"1.1.1-define model purpose & Problem context","Provide a clear statement of the modelling objectives and problem that the model seeks to illuminate. What is the purpose of the model/s? Ensure that you specify any focal taxa and study objectives, as well as any clients for whom the model is developed for. Briefly outline the ecological problem and the decision-problem, including the decision-trigger and any regulatory frameworks relevant to the problem.",""
"4",1,2,"1.2-problem_formulation: specify modelling context","The scope of the model, including temporal and spatial resolutions are defined here [@Mahmoud2009], and any limitations on model development analysis and flexibility should be outlined here [@Jakeman:2006ii].   Note that the modelling context is different from the problem context.",""
"5",1,2,"1.2.1-Identify model interest group","Identify interest groups in the modelling context. This includes clients, and end-users of the model.Who is the model for, who is involved in formulating the model? Who needs buy in?Describe the decision-making context in which the model will be used.",""
"6",1,2,"1.2.2 - Model Scope and Scale","Determine the temporal and spatial scope of the model. Where is the boundary of the modelled system? Everything beyond the boundary and not crossing it is to be ignored within the domain of the model, and everything crossing the boundary is to be treated as external forcing (known/unknown), or else as model outputs (observed, or not, Jakeman et al. 2006).The choice of a model’s boundaries is closely linked to the choice of how finely to aggregate the behaviour within the model (Jakeman et al., 2006) - what is the intended scale and resolution of the model (temporal, spatial or otherwise)?Explain how any key concepts or terms within problem or decision-making contexts, such as regulatory terms, will be operationalised and defined in a biologically meaningful way to answer the research question appropriately? (Should this last step go here or within define conceptual framework?).What is the intended domain of applicability of the model, what is the extent of acceptable extrapolations (Grimm)? This is relevant to model transferability... (similar to a COG statement..)",""
"7",1,2,"1.2.3-Logistical Constraints","Time-frame - When must the model by completed, e.g. to help make a decision?What effort and resources are available for both developing the model and operating the model? What degree of flexibility is required from the model? Might the model need to be quickly reconfigured to explore new scenarios or problems proposed by clients / managers / model-users?",""
"8",1,3,"1.3.1-problem_formulation: explain analytical objectives","How will the model be analysed, what analytical questions will the model be used to answer? Examples from ecological decision-making include: To compare the performance of alternative management actions under budget constraint [@Fraser:2017jf]. To search for ‘robust’ decisions under model uncertainty [@McDonald-Madden2008]. To choose the conservation policy that minimises uncertainty [insert ref]. See other examples in Moallemi et al. 2019 & ask Libby and others.   ",""
"9",1,3,"1.3.1-Explain analytical objectives","Explain the analytical objectives",""
"10",1,3,"1.3.2-Identify outcomes to evaluate decisions under scenarios","The outcomes should speak directly to the analytical objectives identified in 1.3.1. Outcomes could be qualitative or quantitative, and include descriptive statistic measures, satisficing measures or regret measures (Moallemi et al. 2019). For example, a model outcome in the top 10th percentile of worst conditions, or some minimum performance threshold for meeting a performance objective, or choosing a decision with the minimum regret or least amount of uncertainty.",""
"11",1,4,"1.4-problem_formulation: define candidate actions decisions","Candidate decisions should be investigated and are specified a priori. Depending on the modelling context, they may be specified by stakeholders, model users or the analyst[@Moallemi2019]. Describe the method used to identify relevant management actions.",""
"12",1,5,"1.5-problem_formulation: specify scenarios","Specify scenarios under which decisions are investigated. Scenarios should be set a priori (i.e. before the model is built, [@Moallemi2019]) and may be stakeholder-defined or expert judgment-driven [@Mahmoud2009]. ",""
"13",2,0,"2.0-define_conceptual_model","Define Conceptual Model - conceptual models underpin the formal or quantitative model (Cartwright et al. 2016). The conceptual model describes the biological mechanisms relevant to the ecological problem and should capture basic premises about how the target system works, including any prior knowledge and assumptions about system processes. Conceptual models may be representeed in a variety of formats, such as influence diagrams, linguistic model block diagram or bond graphs, and these illustrate how model drivers are linked to both outputs or observed responses, and internal (state) variables (Jakeman, Letcher, and Norton, 2006).",""
"14",2,1,"2.1-define_conceptual_model: choose elicitation and representation method","Describe what method you will use to elicit or identify the conceptual model. Some common methods include interviews, drawings, and mapping techniques including influence diagrams, cognitive maps and Bayesian belief networks (Moon et al. 2019). (Libby, to provide link to any structured expert elicitation methods).While itt is difficult to decide and justify which method is most appropriate, however Moon et al. (2019) provide guidance addressing this methodological question. Finally, how do you intend on representing the final conceptual model? This will likely depend on the method chosen to elicit the conceptual model.",""
"15",2,2,"2.2:define_conceptual_model: specify key assumptions uncertainties","This step should list and explain the critical conceptual design decisions, including: “spatial and temporal scales, selection of entities and processes, representation of stochasticity and heterogeneity, consideration of local versus global interactions, environmental drivers, etc.” [@Grimm:2014es]. The influence of particular theories, concepts, or importantly, earlier models, should be explained and justified against alternative conceptual design decisions that might lead to alternative model structures [@Grimm:2014es].Specify key assumptions and uncertainties underlying the model’s design. Describe how uncertainty and variation will be represented in this model. This includes both exogenous uncertainties affecting the system, parametric uncertainty in input data and structural / conceptual nonparametric uncertainty in the model [@Moallemi2019]. ",""
"16",2,3,"2.3-define_conceptual_model: identify predictor response variables","Identify and define system variables structures).: i) What variables will support this decision or action (things we can control). ii) What additional variables may interact with this system (things we can’t control, but can hopefully measure). iii) In what ways do we expect these variables to interact (model structures). The identification and definition of primary model input variables should be driven by scenario definitions, and by the scope of the model described in the problem formulation phase [@Mahmoud2009].",""
"17",2,4,"2.4-define_conceptual_model: prior knowledge data specification and evaluation","Collect, process and prepare data available for parameterisation, determining model structure, and for scenario analysis.",""
"18",2,4,"2.4.1-Collate available data sources that could be used to parameterise or structure the model","Document the identity, quantity and provenance of any data used to develop, identify and test the model. Describe how the data is arranged, in terms of replication and covariates. Explain how you will summarise what data and knowledge will be used to designa and parameterise the model, including references, data sources, and information about whom, when and where those data were collected [@Grimm:14es]",""
"19",2,4,"2.4.2-Data Processing and Preparation","Describe any data preparation and processing steps, including manipulation of environmental layers, e.g. standardisatino and geographic projection.",""
"20",2,4,"2.4.3-Describe any data exploration or preliminary data analyses.","In most modelling cases, it is necessary to perform preliminary analyses to understand the data and check that assumptions and requirements of the chosen modelling procedures are met. Data exploration prior to model fitting or development may include exploratory analyses to check for collinearity, spatial and temporal coverage, quality and resolution, outliers, or the need for transformations [@Yates2018]. Describe how you will summarise and explore your data, and explain what method you will use (graphical, tabular or otherwise) to represent your data and any analyses.",""
"21",2,4,"2.4.4-Data Evaluation","Describe how you will evaluate the quality and sources of both numerical and qualitative data that available for model parameterisation, and for determining the overall model structure [@Grimm:2014es]. Ideally, model input data should be internally consistent across temporal and spatial scales and resolutions, and appropriate to the problem at hand [@Mahmoud2009]. Document any issues with data reliability. This is important because data quality and ecological relevance might be constrained by measurement error, inappropriate experimental design, and heterogeneity and variability inherent in ecological systems [@Grimm:2014es].",""
"22",2,5,"2.5-define_conceptual_model: conceptual model evaluation","Describe how the model will be critically evaluated. Evaluation includes both the overall model structure, and any submodels if relevant. How will any simplifying assumptiosn be critically assessed[@Augusiak:2014gz]? Explain whether this process will include consultation or feedback from a client, manager, or model user.",""
"23",2,6,"2.6-define_conceptual_model: iterative link","Do we need to include any PRT items that speak to expert elicitation or not?",""
"24",3,0,"3.0-formalise_specify_model","Formalise and Specify Model - Critical decisions are made here about the type of model, modelling framework and approach to be used. The model is formalised into a mathematical / statistical description of the system and respective changes it can experience (Mahmoud et al. 2009). In this section describe what quantitative methods you will use to build the model/s, explain how they are relevant to the client/manager/user’s purpose.",""
"25",3,0,"3.2-Choose model features and family","All modelling approaches require the selection of model features, which conform with the conceptual model and data specified in previous steps (Jakeman 2006). The choice of model are determined in conjunction with features are selected.Usually difficult to change fundamental features of a model beyond an early stage of model development, so careful thought and planning here is useful to the modeller (Jakeman, 2006). However, if changes to these fundamental aspects of the model do need to change, please document how and why these choices were made.",""
"26",3,1,"3.1-formalise_specify_model: choose model class, framework and approach","Describe what class or approach of model you will use and explain how the choice of model class was informed by the analytical objectives of the model. Modelling approaches lie on a spectrum from correlative or phenomenological to mechanistic or process-based  [@Yates2018]; where correlative models use mathematical functions fitted to data to describe underlying processes, and mechanistic models explicitly represent processes and details of component parts of a biological system that are expected to give rise to the data [@White2019a].",""
"27",3,2,"3.2.1-Explain how you will operationalise response variable(s)","Specify how you will operationalise response variables in the model. This should relate directly to the analytical and or management objectives specified during the problem formulation phase. Specifications could include:- qualitative change, such as a direction of change- the extent of a response- an extreme value- a trend- a long-term mean- a probability distribution- a spatial pattern- a time-series, or- the frequency, location, or probability of some event occuring.Provide a rationale for your choices, including why plausible alternatives were not chosen (Jakeman, 2006).",""
"28",3,2,"3.2.3-Choose model family","Specify which family of statistical distributions you will use in your model, and describe any transformations, or link functions. Justify your decision based on the purpose, objectives, prior knowledge and logistical constraints (Jakeman, 2006) specified in the problem formulation phase. Include in your rationalse for selection detail about which variables the model outputs are sensitive to, what aspects of their behaviour are important, and any associated spatial or temporal dimensions in sampling.",""
"29",3,2,"3.2.3-Choose model features","Specify which types of variables are covered in the model, and the nature of their treatment (e.g. lumped/distributed, linear/non-linear, stochastic/deterministic, Jakeman, 2006). Specify model structural features, including:- the functional form of interactions,- data structures- measures used to specify links, spatio temporal scales and processes as well as their interactions- any bins or discretisation of continuous variables",""
"30",3,3,"3.3-formalise_specify_model: choose how to find structure and parameters","Choice of methods for finding model structure and parameters “Jakeman, 2006""Structure Determination: structure may be knowledge supported or data-driven [@Boets:2015gl]. Data-driven structure algorithms could include supervised or unsupervised machine learning, and modellers could choose from algorithms including decision-tree, K-nearest neighbour, or cluster algorithms, [@Liu2018b], for example.Parameter Estimation: Expert-judgment [@Czembor2011], ….<>",""
"31",3,4,"3.4-Choose approach for identifying model structure and parameters","“The present step addresses the Iterative process of finding a suitable model structure and parameter values.”  [@Jakeman:2006ii]. I think that recent model refinement / taxonomy paper describes this step.Ensemble: single model, multi-model average [@Yates2018].“the complexity of interactions proposed for the model may be increased or reduced, according to the results of model testing).“Can some system descriptors, for instance dimensionality and processes, […] be aggregated to make the representation more efficient, worrying about only what dominates the response of the system at the scales of concern.” [@Jakeman:2006ii].",""
"32",3,5,"3.5-formalise_specify_model: choose estimation performance criteria and technique","Before calibrating the model to the data, the performance criteria on which the calibration is judged are chosen. These criteria and their underlying assumptions should reflect the desired properties of the parameter estimates / structure [@Jakeman:2006ii]. For example, modellers might seek that parameter estimates are robust to outliers, unbiased, and yield appropriate predictive performance. Modellers will need to consider whether the assumptions of the estimation technique yielding those desired criteria are appropriate to the problem at hand. For integrated or sub-divided models, other considerations might include choices about where to disaggregate the model for parameter estimation; e.g. spatial sectioning (streams into reaches) and temporal sectioning (piece-wise linear models) [@Jakeman:2006ii].    ",""
"33",3,6,"3.6-formalise_specify_model: specify formal model","Why should this be the last step?? Given that the preceding step\s apply to the act of fitting rather than to model specification (seemingly). Well, the way the model is specified is dictated by the previous choices.Once critical decisions have been made about the approach and method of model specification, specify formal model(s).",""
"34",3,7,"3.7-formalise_specify_model: Specify model assumptions and uncertainties","How to model assumptions and uncertainties differ from those in the conceptual model?",""
"35",4,0,"4.0-model_calibration_fitting_checking","Model Calibration, Model Fitting & Checking - this phase involves fitting the formally defined model to data. There are many implementatino techniques available to fit these models. Decisions must be made about how to partition the data into training and tests sets, if possible.Should assumption violation checks go in model checking, i.e. phase 4, or do these go in model validation and evaluation? Check the GMP literature on this.",""
"36",4,1,"4.1-model_calibration_fitting_checking: Choose model calibration and validation scheme","Internal or externalcross-validation may be implemented [@Yates2018]. Typically testing onindependent datasets is not possible for large and/or integrated models,especially when they are being used to generate anticipatory predictions beyondthe conditions on which they were calibrated [@Jakeman:2006ii]. Follow-updecisions, among others, include how many folds, and what method ofpartitioning should be used?",""
"37",4,2,"4.2-model_calibration_fitting_checking: choose performance criteria","Correlation score,coefficient of determination, specificity, sensitivity, AUC, ROC, etc.",""
"38",4,3,"4.3-model_calibration_fitting_checking: Implementation verification","“(1) Whether the computer codeimplementing the model has been thoroughly tested for programming errors, (2)whether the implemented model performs as indicated by the model description,and (3) how the software has been designed and documented to provide necessaryusability tools (interfaces, automation of experiments, etc.) and to facilitatefuture installation, modification, and maintenance.” [@Grimm:2014es]I think I had this separate from other model checking steps because in my mind checking step 1 has a distinct purpose from other model checking and diagnostic procedures. I think it’s probably just best to include a “model checking” step and to include ‘implementation verification’ as its own specific item in this group of items.",""
"39",4,4,"4.4-model_calibration_fitting_checking: model checking","Unclear to me how this differs from my description in implementation verification.Should assumption violation checks go in model checking, i.e. phase 4, or do these go in model validation and evaluation? Check the GMP literature on this.",""
"40",4,5,"4.5-model_calibration_fitting_checking: model fitting and calibration","So whilst this is a step in a modelling workflow, it doesn’t exactly make sense to include this as an item in the PRT itself. What we could put for this step, and for other similar steps that basically involve the implementation or inaction of all the preceding decisions in a phase, is to have a step in the template that allows users to state the results of the checking etc. and any resultant decisions. What will be done if the model fails… I don’t think that can be answered a-priori, so it’s likely that a pre-registration for models involves multiple time-stampsed versions. 1. Where All decisions are made assuming ideal outcomes, i.e. where all model tests and checking pass, and everything is straight forward - NO refinement of model is needed. 2. Where refinement of model is needed based on results of first round tests… Still don’t know how to capture this in a PRT. and whether it’s possible to have a “living preregistration” for models…. Does this just defeat the purpose of a preregistration?? Maybe we get users to add amendments if round 1 of checking or evaluation requires refinement. ",""
"41",5,0,"5.0-model_validation_evaluation","Model Validation & Evaluation - This phase consists of a suite of analyses that collectively inform the decision and whether and when a model is suitable to meet its intended purpose (Augusiak, Van den Brink and Grimm 2014). Errors in design and implementation of the model and their implication on the model output are assessed. Ideally independent data is used against the model outputs to assess whether the model output behaviour exhibits the required accuracy for the model’s intended purpose. The outcomes of these analyses build confidence in the model applications and increase understandingof model strengths and limitations.",""
"42",5,1,"5.1-model_validation_evaluation: model output verification","“(1) How well model output matches observations and (2) how much calibration and effects of environmental drivers were involved in obtaining good fits of model output and data.” This is just calibration right?Jakeman 2006.",""
"43",5,2,"5.2-model_validation_evaluation: model analysis","“(1) How sensitive model output is to changes in model parameters (sensitivity analysis), and (2) how well the emergence of model output has been understood.” [@Grimm:2014es].",""
"44",5,3,"5.3-model_validation_evaluation: model output corroboration","“How model predictions compare to independent data and patterns that were not used, and preferably not even known, while the model was developed, parameterized, and verified. By documenting model output corroboration, model users learn about evidence which, in addition to model output verification, indicates that the model is structurally realistic so that its predictions can be trusted to some degree.”This is what we typically call “validation”",""
"45",5,4,"5.4-model_validation_evaluation: choose metric and performance criteria","I think this should be shifted to problem formulation… where I originally had this item… described here as:""An a priori method or methods of evaluating and distinguishing the performance of scenario outcomes is necessary. The performance measures link back to the analytical objectives. Examples include descriptive statistic measures, such as a model in the top 10th percentile of worst conditions, satisficing measures, e.g. a minimum performance threshold for achieving a performance objective, regret measures, e.g. choosing a decision with minimum regret [@Moallemi2019].”Also… I think this description by Moallemi actually pertains to the model application part.. not the model evaluation part. Model evaluation is still answering the question “Do we have a useful/credible/feasible model""",""
"46",5,5,"5.5-model_validation_evaluation: evaluate model assumptions","What tests, analyses or visualisations will be performed to evaluate whether model assumptions have been violated? Violation of the theoretical and statistical assumptions of a particular model can lead to unreliable results (37, 134, 137, 138) for model interpretation, geographic predictions, and projections (38, 50). Demonstrating that no model assumptions were violated is a gold standard in modelling. In cases where a researcher tests assumptions and finds departures from them, it is necessary to assess the consequences on interpretation of the results. If violation of assumptions cannot be avoided, explicit exploration and discussion of consequences for the interpretation of results in the particular context in which they are being used represents the bronze standard (25, 50). Blindly using models without testing assumptions should be considered a deficient practice. ” [@Araujo2019]. What tests, analyses or visualisations will be performed to evaluate whether model assumptions have been violated?   ",""
"47",5,6,"5.6-model_validation_evaluation: evaluate model outputs","Test different aspects of the model, such as realism, generality and accuracy.   Ideally more than one performance measure is testedPerformance assessment might include a suite ofestimates, including tests of “accuracy, bias, calibration, discrimination,refinement, resolution, and skill (157)” [@Araujo2019].",""
"48",5,7,"5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis","“As regards what method should be used, our preference is for methods which are exploratory, model-independent, able to capture interactions and to treat a group of factors. A carefully performed un- certainty analysis, followed by sensitivity analysis, is an important in- gredient of the quality assurance of a model as well as a necessary condition for any model-based analysis or inference.” [@Saltelli2019].Uses of sensitivity analysis: “Sensitivity analysis is used for many purposes. Primarily it is used as a tool to quantify the contributions of model inputs, or sub-groups of inputs, to the uncertainty in the model output” … “In this uncertainty setting, typical objectives are to identify which input factors contribute the most to model uncertainty (“factor prioritisation”) so that further information might be collected about these parameters to reduce model uncertainty, or to identify factors which contribute very little and can potentially be !xed (“factor !xing”) (Saltelli and Tarantola, 2002).” … “Other applications that are not necessarily related to uncertainty are for example in engineering design, where “design sensitivity analysis” is used as a tool for structural optimisation (Allaire et al., 2004). Sensi- tivity analysis can also be used to better understand processes within models, and thereby, the natural systems on which they are based (Becker et al., 2011), or as a quality assurance tool: an unexpected strong dependence of the output upon an input deemed irrelevant might either illuminate the analyst on an unexpected feature of the system or reveal a conceptual or coding error.”“sensitivity analysis is “the study of how the un- certainty in the output of a model (numerical or otherwise) can be apportioned to di""erent sources of uncertainty in the model input” (Saltelli, 2002).” [@Saltelli2019].“uncertainty analysis (UA), which, as we de!ne it here, characterizes the uncertainty in model prediction, without identifying which assump- tions are primarily responsible.” [@Saltelli2019].“Characterising the output distribution – e.g. by constructing it em- pirically from the output data points, constitutes an uncertainty ana- lysis. The UA may also involve extracting summary statistics, such as the mean, median, and variance, from this distribution and possibly by assigning con!dence bounds, e.g. on the mean.Once this is done, the next step could be to use sensitivity analysis to assign this uncertainty to the input factors. Sensitivity analysis allows us to infer that, for example, “this factor alone is responsible for 70% of the uncertainty in the output”. “[@Saltelli2019].Item: What experiments (sensitivity analysis or simulation experiments) will you perform to increase understanding of the model …..",""
"49",6,0,"6.0-model_application","I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",""
"50",6,1,"6.1-model_application: choose performance criteria for scenario analysis","I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",""
"51",6,2,"6.2-model_application: expert / client / stakeholder evaluation","I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",""
"52",6,3,"6.3-model_application: Scenario analysis and simulation","I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",""
"53",NA,0,"data_exploration","I suspect this phase should go after defining the conceptual model and before formalising and specifying the full model quantitatively.BUT perhaps, this goes under the “PRIOR KNOWLEDGE< DATA SPECIFICATION AND EVALUATION” node… Se code comment.",""
"54",NA,0,"documentation_manuscript-7","For modellers and analysts working with stakeholders and clients etc, there is an extra step in here about reporting during the research.",""
