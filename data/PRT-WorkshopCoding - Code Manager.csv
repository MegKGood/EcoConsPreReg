Color,Name,Groundedness,Density,Groups,Number of Groups,Comment,Creator,Creation Date,Modifier,Modification Date
●,1-problem_formulation,2,5,"coded_phase, PRT item",2,"Problem Formulation - specifies the decision-making context in which the model will be used, the clients driving model development or stakeholders addressed by the model. It also includes specification of model outputs, statements of the domain of applicability of the model, as well as the extent by which model outputs may be acceptably extrapolated (Grimm et al. 2014).",egould,23/4/20,egould,25/4/20
●,1.1-problem_formulation: define model purpose,9,1,"coded_decision_step, PRT item",2,"Defining the purpose of the model is critical because the model purpose influences choices at later stages of model development [@Jakeman:2006ii]. Common model purposes in ecology include: gaining a better qualitative understanding of the target system, knowledge synthesis and review, and providing guidance for management and decision-making [@Jakeman:2006ii] (could also see Addison paper on the use of models). Note that modelling objectives are distinct from the analytical objectives of the model.    ",egould,25/4/20,egould,27/4/20
●,1.2-problem_formulation: specify modelling context,10,1,"coded_decision_step, PRT item",2,"The scope of the model, including temporal and spatial resolutions are defined here [@Mahmoud2009], and any limitations on model development analysis and flexibility should be outlined here [@Jakeman:2006ii].   Note that the modelling context is different from the problem context.",egould,25/4/20,egould,1/5/20
●,1.3.1-problem_formulation: explain analytical objectives,5,1,"coded_decision_step, PRT item",2,"How will the model be analysed, what analytical questions will the model be used to answer? Examples from ecological decision-making include: To compare the performance of alternative management actions under budget constraint [@Fraser:2017jf]. To search for ‘robust’ decisions under model uncertainty [@McDonald-Madden2008]. To choose the conservation policy that minimises uncertainty [insert ref]. See other examples in Moallemi et al. 2019 & ask Libby and others.   ",egould,25/4/20,egould,1/5/20
●,1.4-problem_formulation: define candidate actions decisions,2,1,"coded_decision_step, PRT item",2,"Candidate decisions should be investigated and are specified a priori. Depending on the modelling context, they may be specified by stakeholders, model users or the analyst[@Moallemi2019]. Describe the method used to identify relevant management actions.",egould,25/4/20,egould,1/5/20
●,1.5-problem_formulation: specify scenarios,2,1,"coded_decision_step, PRT item",2,"Specify scenarios under which decisions are investigated. Scenarios should be set a priori (i.e. before the model is built, [@Moallemi2019]) and may be stakeholder-defined or expert judgment-driven [@Mahmoud2009]. ",egould,25/4/20,egould,1/5/20
●,2.0-define_conceptual_model,12,5,"coded_phase, PRT item",2,"Define Conceptual Model - conceptual models underpin the formal or quantitative model (Cartwright et al. 2016). The conceptual model describes the biological mechanisms relevant to the ecological problem and should capture basic premises about how the target system works, including any prior knowledge and assumptions about system processes. Conceptual models may be representeed in a variety of formats, such as influence diagrams, linguistic model block diagram or bond graphs, and these illustrate how model drivers are linked to both outputs or observed responses, and internal (state) variables (Jakeman, Letcher, and Norton, 2006).",egould,23/4/20,egould,27/4/20
●,2.1-define_conceptual_model: choose elicitation and representation method,3,1,"coded_decision_step, PRT item",2,"Describe what method you will use to elicit or identify the conceptual model. Some common methods include interviews, drawings, and mapping techniques including influence diagrams, cognitive maps and Bayesian belief networks (Moon et al. 2019). (Libby, to provide link to any structured expert elicitation methods).While itt is difficult to decide and justify which method is most appropriate, however Moon et al. (2019) provide guidance addressing this methodological question. Finally, how do you intend on representing the final conceptual model? This will likely depend on the method chosen to elicit the conceptual model.",egould,25/4/20,egould,1/5/20
●,2.2:define_conceptual_model: specify key assumptions uncertainties,6,1,"coded_decision_step, PRT item",2,"This step should list and explain the critical conceptual design decisions, including: “spatial and temporal scales, selection of entities and processes, representation of stochasticity and heterogeneity, consideration of local versus global interactions, environmental drivers, etc.” [@Grimm:2014es]. The influence of particular theories, concepts, or importantly, earlier models, should be explained and justified against alternative conceptual design decisions that might lead to alternative model structures [@Grimm:2014es].Specify key assumptions and uncertainties underlying the model’s design. Describe how uncertainty and variation will be represented in this model. This includes both exogenous uncertainties affecting the system, parametric uncertainty in input data and structural / conceptual nonparametric uncertainty in the model [@Moallemi2019]. ",egould,25/4/20,egould,27/4/20
●,2.3-define_conceptual_model: identify predictor response variables,16,1,"coded_decision_step, PRT item",2,"Identify and define system variables structures).: i) What variables will support this decision or action (things we can control). ii) What additional variables may interact with this system (things we can’t control, but can hopefully measure). iii) In what ways do we expect these variables to interact (model structures). The identification and definition of primary model input variables should be driven by scenario definitions, and by the scope of the model described in the problem formulation phase [@Mahmoud2009].",egould,25/4/20,egould,27/4/20
●,2.4-define_conceptual_model: prior knowledge data specification and evaluation,14,1,"coded_decision_step, PRT item",2,"Collect, process and prepare data available for parameterisation, determining model structure, and for scenario analysis. ",egould,25/4/20,egould,27/4/20
●,2.5-define_conceptual_model: conceptual model evaluation,3,1,"coded_decision_step, PRT item",2,"Describe how the model will be critically evaluated. Evaluation includes both the overall model structure, and any submodels if relevant. How will any simplifying assumptiosn be critically assessed[@Augusiak:2014gz]? Explain whether this process will include consultation or feedback from a client, manager, or model user.",egould,25/4/20,egould,27/4/20
●,2.6-define_conceptual_model: iterative link,4,0,"coded_decision_step, PRT item",2,Do we need to include any PRT items that speak to expert elicitation or not?,egould,25/4/20,egould,27/4/20
●,3.0-formalise_specify_model,3,6,"coded_phase, PRT item",2,"Formalise and Specify Model - Critical decisions are made here about the type of model, modelling framework and approach to be used. The model is formalised into a mathematical / statistical description of the system and respective changes it can experience (Mahmoud et al. 2009). In this section describe what quantitative methods you will use to build the model/s, explain how they are relevant to the client/manager/user’s purpose.",egould,23/4/20,egould,27/4/20
●,"3.1-formalise_specify_model: choose model class, framework and approach",8,1,"coded_decision_step, PRT item",2,"Describe what class or approach of model you will use and explain how the choice of model class was informed by the analytical objectives of the model. Modelling approaches lie on a spectrum from correlative or phenomenological to mechanistic or process-based  [@Yates2018]; where correlative models use mathematical functions fitted to data to describe underlying processes, and mechanistic models explicitly represent processes and details of component parts of a biological system that are expected to give rise to the data [@White2019a].",egould,25/4/20,egould,27/4/20
●,3.2-Choose model features and family,7,1,PRT item,1,"All modelling approaches require the selection of model features, which conform with the conceptual model and data specified in previous steps (Jakeman 2006). The choice of model are determined in conjunction with features are selected.Usually difficult to change fundamental features of a model beyond an early stage of model development, so careful thought and planning here is useful to the modeller (Jakeman, 2006). However, if changes to these fundamental aspects of the model do need to change, please document how and why these choices were made.",egould,7/5/20,egould,7/5/20
●,3.3-formalise_specify_model: choose how to find structure and parameters,1,2,"coded_decision_step, PRT item",2,"Choice of methods for finding model structure and parameters “Jakeman, 2006""Structure Determination: structure may be knowledge supported or data-driven [@Boets:2015gl]. Data-driven structure algorithms could include supervised or unsupervised machine learning, and modellers could choose from algorithms including decision-tree, K-nearest neighbour, or cluster algorithms, [@Liu2018b], for example.Parameter Estimation: Expert-judgment [@Czembor2011], ….<>",egould,25/4/20,egould,7/5/20
●,3.4-Choose approach for identifying model structure and parameters,6,2,"coded_decision_step, PRT item",2,"“The present step addresses the Iterative process of finding a suitable model structure and parameter values.”  [@Jakeman:2006ii]. I think that recent model refinement / taxonomy paper describes this step.Ensemble: single model, multi-model average [@Yates2018].“the complexity of interactions proposed for the model may be increased or reduced, according to the results of model testing).“Can some system descriptors, for instance dimensionality and processes, […] be aggregated to make the representation more efficient, worrying about only what dominates the response of the system at the scales of concern.” [@Jakeman:2006ii].",egould,27/4/20,egould,28/4/20
●,3.5-formalise_specify_model: choose estimation performance criteria and technique,1,2,"coded_decision_step, PRT item",2,"Before calibrating the model to the data, the performance criteria on which the calibration is judged are chosen. These criteria and their underlying assumptions should reflect the desired properties of the parameter estimates / structure [@Jakeman:2006ii]. For example, modellers might seek that parameter estimates are robust to outliers, unbiased, and yield appropriate predictive performance. Modellers will need to consider whether the assumptions of the estimation technique yielding those desired criteria are appropriate to the problem at hand. For integrated or sub-divided models, other considerations might include choices about where to disaggregate the model for parameter estimation; e.g. spatial sectioning (streams into reaches) and temporal sectioning (piece-wise linear models) [@Jakeman:2006ii].    ",egould,25/4/20,egould,7/5/20
●,3.6-formalise_specify_model: specify formal model,6,1,"coded_decision_step, PRT item",2,"Why should this be the last step?? Given that the preceding step\s apply to the act of fitting rather than to model specification (seemingly). Well, the way the model is specified is dictated by the previous choices.Once critical decisions have been made about the approach and method of model specification, specify formal model(s).",egould,25/4/20,egould,7/5/20
●,3.7-formalise_specify_model: Specify model assumptions and uncertainties,3,0,"coded_decision_step, PRT item",2,How to model assumptions and uncertainties differ from those in the conceptual model?,egould,25/4/20,egould,7/5/20
●,4.0-model_calibration_fitting_checking,1,5,"coded_phase, PRT item",2,"Model Calibration, Model Fitting & Checking - this phase involves fitting the formally defined model to data. There are many implementatino techniques available to fit these models. Decisions must be made about how to partition the data into training and tests sets, if possible.Should assumption violation checks go in model checking, i.e. phase 4, or do these go in model validation and evaluation? Check the GMP literature on this.",egould,23/4/20,egould,27/4/20
●,4.1-model_calibration_fitting_checking: Choose model calibration and validation scheme,2,1,"coded_decision_step, PRT item",2,"Internal or externalcross-validation may be implemented [@Yates2018]. Typically testing onindependent datasets is not possible for large and/or integrated models,especially when they are being used to generate anticipatory predictions beyondthe conditions on which they were calibrated [@Jakeman:2006ii]. Follow-updecisions, among others, include how many folds, and what method ofpartitioning should be used?",egould,25/4/20,egould,27/4/20
●,4.2-model_calibration_fitting_checking: choose performance criteria,6,2,"coded_decision_step, PRT item",2,"Correlation score,coefficient of determination, specificity, sensitivity, AUC, ROC, etc.",egould,25/4/20,egould,27/4/20
●,4.3-model_calibration_fitting_checking: Implementation verification,12,2,"coded_decision_step, PRT item",2,"“(1) Whether the computer codeimplementing the model has been thoroughly tested for programming errors, (2)whether the implemented model performs as indicated by the model description,and (3) how the software has been designed and documented to provide necessaryusability tools (interfaces, automation of experiments, etc.) and to facilitatefuture installation, modification, and maintenance.” [@Grimm:2014es]I think I had this separate from other model checking steps because in my mind checking step 1 has a distinct purpose from other model checking and diagnostic procedures. I think it’s probably just best to include a “model checking” step and to include ‘implementation verification’ as its own specific item in this group of items.",egould,25/4/20,egould,27/4/20
●,4.4-model_calibration_fitting_checking: model checking,8,3,"coded_decision_step, PRT item",2,"Unclear to me how this differs from my description in implementation verification.Should assumption violation checks go in model checking, i.e. phase 4, or do these go in model validation and evaluation? Check the GMP literature on this.",egould,25/4/20,egould,27/4/20
●,4.5-model_calibration_fitting_checking: model fitting and calibration,3,1,"coded_decision_step, PRT item",2,"So whilst this is a step in a modelling workflow, it doesn’t exactly make sense to include this as an item in the PRT itself. What we could put for this step, and for other similar steps that basically involve the implementation or inaction of all the preceding decisions in a phase, is to have a step in the template that allows users to state the results of the checking etc. and any resultant decisions. What will be done if the model fails… I don’t think that can be answered a-priori, so it’s likely that a pre-registration for models involves multiple time-stampsed versions. 1. Where All decisions are made assuming ideal outcomes, i.e. where all model tests and checking pass, and everything is straight forward - NO refinement of model is needed. 2. Where refinement of model is needed based on results of first round tests… Still don’t know how to capture this in a PRT. and whether it’s possible to have a “living preregistration” for models…. Does this just defeat the purpose of a preregistration?? Maybe we get users to add amendments if round 1 of checking or evaluation requires refinement. ",egould,25/4/20,egould,27/4/20
●,5.0-model_validation_evaluation,14,7,"coded_phase, PRT item",2,"Model Validation & Evaluation - This phase consists of a suite of analyses that collectively inform the decision and whether and when a model is suitable to meet its intended purpose (Augusiak, Van den Brink and Grimm 2014). Errors in design and implementation of the model and their implication on the model output are assessed. Ideally independent data is used against the model outputs to assess whether the model output behaviour exhibits the required accuracy for the model’s intended purpose. The outcomes of these analyses build confidence in the model applications and increase understandingof model strengths and limitations. ",egould,23/4/20,egould,27/4/20
●,5.1-model_validation_evaluation: model output verification,7,1,"coded_decision_step, PRT item",2,“(1) How well model output matches observations and (2) how much calibration and effects of environmental drivers were involved in obtaining good fits of model output and data.” This is just calibration right?Jakeman 2006.,egould,25/4/20,egould,27/4/20
●,5.2-model_validation_evaluation: model analysis,7,1,"coded_decision_step, PRT item",2,"“(1) How sensitive model output is to changes in model parameters (sensitivity analysis), and (2) how well the emergence of model output has been understood.” [@Grimm:2014es].",egould,25/4/20,egould,29/4/20
●,5.3-model_validation_evaluation: model output corroboration,4,1,"coded_decision_step, PRT item",2,"“How model predictions compare to independent data and patterns that were not used, and preferably not even known, while the model was developed, parameterized, and verified. By documenting model output corroboration, model users learn about evidence which, in addition to model output verification, indicates that the model is structurally realistic so that its predictions can be trusted to some degree.”This is what we typically call “validation”",egould,25/4/20,egould,27/4/20
●,5.4-model_validation_evaluation: choose metric and performance criteria,0,1,"coded_decision_step, PRT item",2,"I think this should be shifted to problem formulation… where I originally had this item… described here as:""An a priori method or methods of evaluating and distinguishing the performance of scenario outcomes is necessary. The performance measures link back to the analytical objectives. Examples include descriptive statistic measures, such as a model in the top 10th percentile of worst conditions, satisficing measures, e.g. a minimum performance threshold for achieving a performance objective, regret measures, e.g. choosing a decision with minimum regret [@Moallemi2019].”Also… I think this description by Moallemi actually pertains to the model application part.. not the model evaluation part. Model evaluation is still answering the question “Do we have a useful/credible/feasible model""",egould,25/4/20,egould,27/4/20
●,5.5-model_validation_evaluation: evaluate model assumptions,1,2,"coded_decision_step, PRT item",2,"What tests, analyses or visualisations will be performed to evaluate whether model assumptions have been violated? Violation of the theoretical and statistical assumptions of a particular model can lead to unreliable results (37, 134, 137, 138) for model interpretation, geographic predictions, and projections (38, 50). Demonstrating that no model assumptions were violated is a gold standard in modelling. In cases where a researcher tests assumptions and finds departures from them, it is necessary to assess the consequences on interpretation of the results. If violation of assumptions cannot be avoided, explicit exploration and discussion of consequences for the interpretation of results in the particular context in which they are being used represents the bronze standard (25, 50). Blindly using models without testing assumptions should be considered a deficient practice. ” [@Araujo2019]. What tests, analyses or visualisations will be performed to evaluate whether model assumptions have been violated?   ",egould,25/4/20,egould,27/4/20
●,5.6-model_validation_evaluation: evaluate model outputs,3,1,"coded_decision_step, PRT item",2,"Test different aspects of the model, such as realism, generality and accuracy.   Ideally more than one performance measure is testedPerformance assessment might include a suite ofestimates, including tests of “accuracy, bias, calibration, discrimination,refinement, resolution, and skill (157)” [@Araujo2019].",egould,25/4/20,egould,27/4/20
●,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,12,1,"coded_decision_step, PRT item",2,"“As regards what method should be used, our preference is for methods which are exploratory, model-independent, able to capture interactions and to treat a group of factors. A carefully performed un- certainty analysis, followed by sensitivity analysis, is an important in- gredient of the quality assurance of a model as well as a necessary condition for any model-based analysis or inference.” [@Saltelli2019].Uses of sensitivity analysis: “Sensitivity analysis is used for many purposes. Primarily it is used as a tool to quantify the contributions of model inputs, or sub-groups of inputs, to the uncertainty in the model output” … “In this uncertainty setting, typical objectives are to identify which input factors contribute the most to model uncertainty (“factor prioritisation”) so that further information might be collected about these parameters to reduce model uncertainty, or to identify factors which contribute very little and can potentially be !xed (“factor !xing”) (Saltelli and Tarantola, 2002).” … “Other applications that are not necessarily related to uncertainty are for example in engineering design, where “design sensitivity analysis” is used as a tool for structural optimisation (Allaire et al., 2004). Sensi- tivity analysis can also be used to better understand processes within models, and thereby, the natural systems on which they are based (Becker et al., 2011), or as a quality assurance tool: an unexpected strong dependence of the output upon an input deemed irrelevant might either illuminate the analyst on an unexpected feature of the system or reveal a conceptual or coding error.”“sensitivity analysis is “the study of how the un- certainty in the output of a model (numerical or otherwise) can be apportioned to di""erent sources of uncertainty in the model input” (Saltelli, 2002).” [@Saltelli2019].“uncertainty analysis (UA), which, as we de!ne it here, characterizes the uncertainty in model prediction, without identifying which assump- tions are primarily responsible.” [@Saltelli2019].“Characterising the output distribution – e.g. by constructing it em- pirically from the output data points, constitutes an uncertainty ana- lysis. The UA may also involve extracting summary statistics, such as the mean, median, and variance, from this distribution and possibly by assigning con!dence bounds, e.g. on the mean.Once this is done, the next step could be to use sensitivity analysis to assign this uncertainty to the input factors. Sensitivity analysis allows us to infer that, for example, “this factor alone is responsible for 70% of the uncertainty in the output”. “[@Saltelli2019].Item: What experiments (sensitivity analysis or simulation experiments) will you perform to increase understanding of the model …..",egould,25/4/20,egould,27/4/20
●,6.0-model_application,1,0,"coded_phase, PRT item",2,"I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",egould,23/4/20,egould,30/4/20
●,6.1-model_application: choose performance criteria for scenario analysis,4,0,"coded_decision_step, PRT item",2,"I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",egould,25/4/20,egould,30/4/20
●,6.2-model_application: expert / client / stakeholder evaluation,7,0,"coded_decision_step, PRT item",2,"I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",egould,25/4/20,egould,30/4/20
●,6.3-model_application: Scenario analysis and simulation,9,0,"coded_decision_step, PRT item",2,"I think this phase might be missing some steps. Namely decisions about how we analyse the model output to answer our research question, how we might graph or plot the model output so that we can meaningfully analyse it and interpret it to answer our research questions / meet the modelling purpose?Liasing with experts, clients and stakeholders once the results from the modellign exercise have been synthesised.",egould,25/4/20,egould,30/4/20
●,data_exploration,5,0,"coded_phase, PRT item",2,"I suspect this phase should go after defining the conceptual model and before formalising and specifying the full model quantitatively.BUT perhaps, this goes under the “PRIOR KNOWLEDGE< DATA SPECIFICATION AND EVALUATION” node… Se code comment.",egould,23/4/20,egould,25/4/20
●,documentation_manuscript-7,9,0,"coded_phase, PRT item",2,"For modellers and analysts working with stakeholders and clients etc, there is an extra step in here about reporting during the research.",egould,23/4/20,egould,25/4/20