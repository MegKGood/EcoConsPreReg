Number,Name,Text Content,Document,Codes,Number of Codes,Comment,Start position,End position,Creator,Creation Date,Modifier,Modification Date
2:5,Translate conceptual model into quantitative model. Decision point: what modelling structure is used…,Translate conceptual model into quantitative model. Decision point: what modelling structure is used. What is in or out of scope. ,case 2,"3.6-formalise_specify_model: specify formal model, decision_point",2,,209,339,egould,21/04/2020,egould,21/04/2020
3:5,Usually at same time as step 2. Collate available data sources that could be used to parameterise th…,Usually at same time as step 2. Collate available data sources that could be used to parameterise the model. ,case 3,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, decision_point",2,,209,318,egould,21/04/2020,egould,21/04/2020
4:5,Use hierarchy of data. - Published literature / grey on species in same location. - Data on same spe…,Use hierarchy of data. - Published literature / grey on species in same location. - Data on same species but different location. - Similar species same or different location but relevant. - expert elicited estimates.,case 4,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, 3.1-formalise_specify_model: choose model class, framework and approach, decision_point",3,,209,425,egould,21/04/2020,egould,21/04/2020
5:5,Parameterise model using various methods depending on data sources. This may include development of…,Parameterise model using various methods depending on data sources. This may include development of statistical models. Evaluate model behaviour given input estimates in collaboration with experts. ,case 5,"4.3-model_calibration_fitting_checking: Implementation verification, decision_point",2,"""Parameterise model using various methods depending on data sources. This may include development of statistical models.""The form of evaluation described here is not checking but evaluation in the commmonky used sense.Highlights the iterative nature of model development",209,407,egould,21/04/2020,egould,21/04/2020
6:5,Identify alternative management alternatives (often with end users). Set up model scenarios for comp…,Identify alternative management alternatives (often with end users). Set up model scenarios for comparison and run.,case 6,"1.4-problem_formulation: define candidate actions decisions, 1.5-problem_formulation: specify scenarios, 6.3-model_application: Scenario analysis and simulation, decision_point",4,"This is interesting that the step was listed in practice as occurring after model development and parameterisation. I think this is important… Moallemi say it should go first, in the problem formulation phase, and I think that it should go there too in order to avoid any sort of QRP’s. However, I guess maybe there are insights that can be gained from the knowledge development process that might inform whether the candidate actions and scenarios are relevant / need to be altered and refined to match the nuances of the model. Perhaps there could be a stage in model evaluation that involves reviewing the specified scenarios and candidate actions.",209,324,egould,21/04/2020,egould,21/04/2020
7:5,Sensitivity analysis. Evaluate model sensitivity using 3 methods. - deterministic sensitivity (vital…,Sensitivity analysis. Evaluate model sensitivity using 3 methods. - deterministic sensitivity (vital rates). - Stochastic sensitivity (variability in the model). - Scenario sensitivity (effect of changes based on scenarios).,case 7,"5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis, decision_point",2,,209,433,egould,21/04/2020,egould,21/04/2020
8:5,Expert Evaluation of of rankings of scenarios and model behaviour.,Expert Evaluation of of rankings of scenarios and model behaviour.,case 8,"6.1-model_application: choose performance criteria for scenario analysis, 6.2-model_application: expert / client / stakeholder evaluation, decision_point",3,"Is this an additional phase or step here? Some at the workshop thought that there was a missing component at the end of the model development cycle, and that was to evaluate with the experts, stakeholders or clients.",209,275,egould,21/04/2020,egould,21/04/2020
9:8,who is this for? Who is involved in formulation? Who needs buy in.,who is this for? Who is involved in formulation? Who needs buy in.,case 9,"1.2-problem_formulation: specify modelling context, group_sheet_decision_point",2,,301,367,egould,21/04/2020,egould,21/04/2020
11:5,"Learn what the questions were originally and what was done, including and constraints with the data.","Learn what the questions were originally and what was done, including and constraints with the data. ",case 11,"1.2-problem_formulation: specify modelling context, 1.3.1-problem_formulation: explain analytical objectives, decision_point, PRT item",4,pre-existing data. Taking over a project.,295,396,egould,21/04/2020,egould,21/04/2020
12:5,"Understand the data: how many samples, how arranged, replication, variation, covariates. Includes so…","Understand the data: how many samples, how arranged, replication, variation, covariates. Includes some data summary figures / tables. Discuss with data collectors.",case 12,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, decision_point",2,,295,458,egould,21/04/2020,egould,21/04/2020
13:5,Design an analysis approach to test . Evaluate the original question that the data are able to suppo…,Design an analysis approach to test . Evaluate the original question that the data are able to support. Discuss with data collectors.,case 13,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, 2.6-define_conceptual_model: iterative link, decision_point",3,,295,428,egould,21/04/2020,egould,21/04/2020
15:12,"Conduct analysis, including confirming correct assumptions are met","Conduct analysis, including confirming correct assumptions are met ",case 15,"4.3-model_calibration_fitting_checking: Implementation verification, 4.4-model_calibration_fitting_checking: model checking",2,,295,362,egould,23/04/2020,egould,23/04/2020
15:13,and then evaluate model fit or performance.,and then evaluate model fit or performance.,case 15,5.6-model_validation_evaluation: evaluate model outputs,1,,362,405,egould,23/04/2020,egould,23/04/2020
18:5,Check final with someone.,Check final with someone.,case 18,"6.2-model_application: expert / client / stakeholder evaluation, decision_point",2,"Again, this might need to be an extra step within the final phase or a new phase - presenting the final model and results to stakeholders, clients, experts, etc.",295,320,egould,21/04/2020,egould,21/04/2020
20:5,Identify variables relevant to the conceptual framework.,Identify variables relevant to the conceptual framework.,case 20,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,,123,179,egould,21/04/2020,egould,21/04/2020
21:5,"Write a draft statistical analysis plan, identify likely analyses / approach to undertake + key feat…","Write a draft statistical analysis plan, identify likely analyses / approach to undertake + key features / outputs + tables.",case 21,"3.1-formalise_specify_model: choose model class, framework and approach, decision_point",2,,123,247,egould,21/04/2020,egould,21/04/2020
22:5,"Data importing, cleaning + query any.","Data importing, cleaning + query any.",case 22,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, decision_point",2,I’ve put this in ‘define conceptual model’ because of the data specification step that is included as one of my suggested decision steps for this workflow.,123,160,egould,21/04/2020,egould,21/04/2020
23:5,"For each analysis, graph the relationship of each ?predictor marginally & then look at interactions.","For each analysis, graph the relationship of each ?predictor marginally & then look at interactions.",case 23,"2.3-define_conceptual_model: identify predictor response variables, 2.6-define_conceptual_model: iterative link, data_exploration, decision_point",4,"Again, I’ve included this exploratory / preliminary plotting in define_coneptual_model for the time-being. But we might need to add an extra step to the phase since this issue has come up on several occasions.Perhaps this involves some back-and-forthing between formalising and specifying the model, and identifying variables… ",123,223,egould,21/04/2020,egould,21/04/2020
24:5,Perform statistical model in SAP (statistical analysis plan).,Perform statistical model in SAP (statistical analysis plan).,case 24,"3.1-formalise_specify_model: choose model class, framework and approach, decision_point",2,,123,184,egould,21/04/2020,egould,21/04/2020
25:12,and assumptions,and assumptions,case 25,"4.3-model_calibration_fitting_checking: Implementation verification, 4.4-model_calibration_fitting_checking: model checking",2,"""Assess statistiacl model for […] assumptions""",156,171,egould,23/04/2020,egould,23/04/2020
26:5,"Revise statistical model based on violations to assumptions, misfit","Revise statistical model based on violations to assumptions, misfit",case 26,"4.5-model_calibration_fitting_checking: model fitting and calibration, decision_point",2,"Again, need extra step insierted at the end of model validation to deal with instances where the model needs to be revised.",123,190,egould,21/04/2020,egould,21/04/2020
27:5,Perform model comparison of key flow metrics,Perform model comparison of key flow metrics,case 27,"6.2-model_application: expert / client / stakeholder evaluation, decision_point",2,Or is this evaluating model outputs?? Need to check this definition here?,123,167,egould,21/04/2020,egould,21/04/2020
28:5,Graphical representat results,Graphical representat results,case 28,"4.0-model_calibration_fitting_checking, 5.2-model_validation_evaluation: model analysis, 6.3-model_application: Scenario analysis and simulation, decision_point",4,"This could equally to apply to all of these three phases, however we could have a step in the PRT’s that asks the researcher to document in the preregistration the expected plots of results that will be drawn.",123,152,egould,21/04/2020,egould,21/04/2020
30:13,Discuss with client,Discuss with client,case 30,6.2-model_application: expert / client / stakeholder evaluation,1,This is where we need an extra step in the model aplication process… working with the client on the final model and analysis outputs.,142,161,egould,23/04/2020,egould,23/04/2020
31:5,Defining key evaluation and research questions. Evaluation questions relate to evaluating responses…,"Defining key evaluation and research questions. Evaluation questions relate to evaluating responses to e-water, research questions to understanding how and why things vary.",case 31,"1.1-problem_formulation: define model purpose, 1.2-problem_formulation: specify modelling context, 1.3.1-problem_formulation: explain analytical objectives, decision_point, PRT item",5,,362,534,egould,21/04/2020,egould,21/04/2020
32:5,Conceptual model being developed to outline predictions -> Justify inclusion / exclusion of predicto…,Conceptual model being developed to outline predictions -> Justify inclusion / exclusion of predictors and covariates. Capture ecologiacl understanding about systems.,case 32,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,"This is an important component for the preregistration — users must justify their inclusion of particular variables under consideration, and explain why they chose to exclude others.It’s interesting that this person has noted that justification is important for a predictive model.. I wonder whether it’s only important in that context.",362,528,egould,21/04/2020,egould,21/04/2020
33:5,Statistical analysis plan deeloped. -> Outline techniques to be used to answer different questions -…,Statistical analysis plan deeloped. -> Outline techniques to be used to answer different questions -> Limitations / constraints of data.,case 33,"3.1-formalise_specify_model: choose model class, framework and approach, decision_point",2,,362,498,egould,21/04/2020,egould,21/04/2020
36:5,Analysis - evaluation of models assumptions etc.,Analysis - evaluation of models assumptions etc.,case 36,"5.6-model_validation_evaluation: evaluate model outputs, decision_point",2,,362,410,egould,21/04/2020,egould,21/04/2020
36:12,models assumptions,models assumptions,case 36,4.4-model_calibration_fitting_checking: model checking,1,,387,405,egould,23/04/2020,egould,23/04/2020
37:5,Evaluation of evidence in light of REQs,Evaluation of evidence in light of REQs,case 37,"6.3-model_application: Scenario analysis and simulation, decision_point",2,Attempt to answer research questions with model and model evaluation output. (I count ‘evidence’ as referring to both of these things). REQ == “research questions”.,361,400,egould,21/04/2020,egould,21/04/2020
38:5,What decisions / actions does the manager / client need to make?,What decisions / actions does the manager / client need to make?,case 38,"1.1-problem_formulation: define model purpose, decision_point",2,Decision trigger & regulatory framework.,152,216,egould,21/04/2020,egould,21/04/2020
39:5,What variables will support this decision / action? (Things we can control),What variables will support this decision / action? (Things we can control),case 39,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,"This could plausibly go into the problem formulation phase, but since we are talking about ‘variables’, i’m going to include this under the conceptual model definition phase.",142,217,egould,21/04/2020,egould,21/04/2020
40:5,What additional variables may interact in this system? (Things we can't control but hopefully measur…,What additional variables may interact in this system? (Things we can't control but hopefully measure),case 40,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,,142,244,egould,21/04/2020,egould,21/04/2020
41:5,In what ways do we expect these variables to interact? (Model structures),In what ways do we expect these variables to interact? (Model structures),case 41,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,,142,215,egould,21/04/2020,egould,21/04/2020
42:5,Are there logistical or feasibility constraints?,Are there logistical or feasibility constraints?,case 42,"1.2-problem_formulation: specify modelling context, decision_point",2,,142,190,egould,21/04/2020,egould,21/04/2020
44:5,How will we assess the quality / goodness of fit of the models?,How will we assess the quality / goodness of fit of the models?,case 44,"4.2-model_calibration_fitting_checking: choose performance criteria, decision_point",2,“Choose Performance Criteria” step.,143,206,egould,21/04/2020,egould,21/04/2020
45:5,"If needed, how will we choose a preferred model or composite model?","If needed, how will we choose a preferred model or composite model?",case 45,"3.4-Choose approach for identifying model structure and parameters, 4.2-model_calibration_fitting_checking: choose performance criteria, decision_point",3,I’ve included it here due to Jakeman’s note about this step: “this step ideally involves hypothesis testing of alternative model structures.,143,210,egould,21/04/2020,egould,21/04/2020
46:5,How will we identify which model components are significant / meaningful?,How will we identify which model components are significant / meaningful?,case 46,"4.2-model_calibration_fitting_checking: choose performance criteria, decision_point",2,"This is important - because it involves pre-specifying how we will interpret the fitted or quantitative model to avoid cognitive biases like confirmation bias, HARKing or other questionable research practices.",143,216,egould,21/04/2020,egould,21/04/2020
47:5,"What is the gap between this model conception and the real-world problem, & what biases might this i…","What is the gap between this model conception and the real-world problem, & what biases might this introduce?",case 47,"3.7-formalise_specify_model: Specify model assumptions and uncertainties, decision_point",2,"This is a really good point, and I think a good place to answer this question would be at the end of ‘specify formal model'",143,252,egould,21/04/2020,egould,21/04/2020
48:5,How are uncertainty & variation represented in this model?,How are uncertainty & variation represented in this model?,case 48,"2.2:define_conceptual_model: specify key assumptions uncertainties, 3.7-formalise_specify_model: Specify model assumptions and uncertainties, decision_point, documentation_manuscript-7",4,"Not quite sure which phase this speaks to. I’ve included it under formalise and specify model, because I think answering this question can aid design-choices when transforming the conceptual model to the quantitative model and choosing how to operationalise and represent these variables.",143,201,egould,21/04/2020,egould,21/04/2020
49:5,What is the risk attitude within this management system?,What is the risk attitude within this management system?,case 49,"6.1-model_application: choose performance criteria for scenario analysis, decision_point",2,"And why is this important? Does it inform how you specify the formal model, or is it more important for eavluating the model analysis outputs?",143,199,egould,21/04/2020,egould,21/04/2020
50:5,What advice / support can we offer to the manager / decision-maker based on our findings?,What advice / support can we offer to the manager / decision-maker based on our findings?,case 50,"6.2-model_application: expert / client / stakeholder evaluation, 6.3-model_application: Scenario analysis and simulation, decision_point",3,"How do we interpret the model outputs? Should there be some a priori decisions about how we interpret the model findings, and how does case 49 about the risk attitude inform how the analyst interprets the model findings?",143,232,egould,21/04/2020,egould,21/04/2020
52:5,Determine spatial and temporal scales required.,Determine spatial and temporal scales required.,case 52,"1.2-problem_formulation: specify modelling context, decision_point",2,,288,335,egould,21/04/2020,egould,21/04/2020
53:8,Expectation management (difference between applied science & science),Expectation management (difference between applied science & science),case 53,"1.2-problem_formulation: specify modelling context, group_sheet_decision_point",2,Expectation management… need further clarification on this. may consult my thematic analysis and workshop scribe notes on this. Or get back to co-authors. Can proper scoping and problem specification aid in this? ANd in ensuring the model-user / client gets what they need from the modelling exercise?,379,448,egould,21/04/2020,egould,21/04/2020
54:5,"Develop conceptual model linking ""waterway health"" and ""deterioration"" to natural variability in wat…","Develop conceptual model linking ""waterway health"" and ""deterioration"" to natural variability in waterway health etc.",case 54,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,Proper problem specification involving careful thought and definition about key terms can likely help with defining the conceptual model appropriately. ,288,405,egould,21/04/2020,egould,21/04/2020
55:5,"Identify possible indicators (and their individual notions of ""deterioration"")","Identify possible indicators (and their individual notions of ""deterioration"")",case 55,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,"I was unclear as to whether this constitutes problem formulation phase or the specification of the conceptual model. Or even the process of taking the conceptual model to the formal model…, i.e. operationalising variables according to whether we can measure and quantify them meaningfully.",288,366,egould,21/04/2020,egould,21/04/2020
56:5,trawl for data,trawl for data,case 56,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, decision_point",2,"“Prior knowledge, data specification and evaluation""",288,302,egould,21/04/2020,egould,21/04/2020
57:5,Build statewide dataset,Build statewide dataset,case 57,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, decision_point",2,"“Prior knowledge, Data specification and evaluation""",288,311,egould,21/04/2020,egould,21/04/2020
58:5,Build feasible statistical models,Build feasible statistical models,case 58,"3.6-formalise_specify_model: specify formal model, decision_point",2,"What does “feasible” mean in this case? Workign within the bounds of logistical and feasibility constraints identified in the problem formulation phase?""",288,321,egould,21/04/2020,egould,21/04/2020
59:5,Run models for all inidcators (in all river basins),Run models for all inidcators (in all river basins),case 59,"4.5-model_calibration_fitting_checking: model fitting and calibration, decision_point",2,,288,339,egould,21/04/2020,egould,21/04/2020
60:5,"Idenity broad statistical model structure, including specification for tests of ""deterioration"" and…","Idenity broad statistical model structure, including specification for tests of ""deterioration"" and ""re?? Related to flow"".",case 60,"3.6-formalise_specify_model: specify formal model, decision_point",2,,288,411,egould,21/04/2020,egould,21/04/2020
60:12,"including specification for tests of ""deterioration"" and ""re?? Related to flow""."," including specification for tests of ""deterioration"" and ""re?? Related to flow"".",case 60,6.1-model_application: choose performance criteria for scenario analysis,1,What are our performance measures for   analysing and evaluating the model outputs / scenario analysis? How do we decide whether our variables of interest have declined in quality or not?,330,411,egould,23/04/2020,egould,23/04/2020
61:5,"Extract specific tests on ""deterioration"" and ""rec? related to flow""","Extract specific tests on ""deterioration"" and ""rec? related to flow""",case 61,"6.3-model_application: Scenario analysis and simulation, decision_point",2,,288,356,egould,21/04/2020,egould,21/04/2020
62:5,Provide synthesis at higher level than every riser basin x indicator combinatino,Provide synthesis at higher level than every riser basin x indicator combinatino,case 62,"6.3-model_application: Scenario analysis and simulation, decision_point",2,,288,368,egould,21/04/2020,egould,21/04/2020
63:5,Identify key outliers especially strongest examples of deterioration,Identify key outliers especially strongest examples of deterioration,case 63,"6.3-model_application: Scenario analysis and simulation, decision_point",2,,288,356,egould,21/04/2020,egould,21/04/2020
64:5,Consult a range of stakeholders through a series of workshops,Consult a range of stakeholders through a series of workshops,case 64,"6.1-model_application: choose performance criteria for scenario analysis, 6.2-model_application: expert / client / stakeholder evaluation, decision_point",3,,288,349,egould,21/04/2020,egould,21/04/2020
69:5,"Variable selection within expert groups. 3x a priori woodland types. Floodplains, grassy woodland, o…","Variable selection within expert groups. 3x a priori woodland types. Floodplains, grassy woodland, obligate seeders. Question: do these states occur in your geographic area of expertise? Do these transitions occur?",case 69,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,,339,553,egould,21/04/2020,egould,21/04/2020
70:5,"Collating transition models - by group, by area. Counts of number of times transition + state occurr…","Collating transition models - by group, by area. Counts of number of times transition + state occurred across models. Reconvene as a group.",case 70,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,,339,478,egould,21/04/2020,egould,21/04/2020
71:5,"Transition by type analysis - graphical analysis. For each transiiton, what was its distribution acr…","Transition by type analysis - graphical analysis. For each transiiton, what was its distribution across functioal types? Proportion of people's STM's in which the transition occurred. How likely is the transition (elicited) -> average.",case 71,"2.3-define_conceptual_model: identify predictor response variables, decision_point",2,,339,574,egould,21/04/2020,egould,21/04/2020
72:5,"States by Type analysis. Again, look at the proportion of ppl's models in which this occurred. How l…","States by Type analysis. Again, look at the proportion of ppl's models in which this occurred. How likely is the state (elicit): sum + average.",case 72,"3.6-formalise_specify_model: specify formal model, decision_point",2,What aggregation measures will you use to analyse your data?,339,482,egould,21/04/2020,egould,21/04/2020
73:5,Discretising / threshold x state and transition. If below 50% then discount. -> But remember there w…,Discretising / threshold x state and transition. If below 50% then discount. -> But remember there was an issue about how we leicited the (verbal probabilities) & what they meant.,case 73,"3.6-formalise_specify_model: specify formal model, decision_point",2,These items from the woodland modelling project might be useful for discretising items in the ‘define conceptual model’ part of the template.Will you discretise any variables? How will you discretise them?,339,518,egould,21/04/2020,egould,21/04/2020
75:8,"Conceptual model in 3 parts: controllable variables, uncontrollable variables, variables that intera…","Conceptual model in 3 parts: controllable variables, uncontrollable variables, variables that interact.",case 75,"2.3-define_conceptual_model: identify predictor response variables, group_sheet_decision_point",2,,248,351,egould,21/04/2020,egould,21/04/2020
76:8,"Often described in different ways: pictures, words, etc.","Often described in different ways: pictures, words, etc.",case 76,"2.1-define_conceptual_model: choose elicitation and representation method, group_sheet_decision_point",2,"This step could also refer to the way in which model variables and structure will be identified, not just how we choose to elicit ‘mental’ models. E.g. we would need preregistration users to specify that they were going to use a ‘naive bayes classification’ model, for example, to find model structure, or some structure learning algorithm.",163,219,egould,21/04/2020,egould,21/04/2020
77:8,Should be / is often done in conjunction with client / manager / user,Should be / is often done in conjunction with client / manager / user,case 77,"2.2:define_conceptual_model: specify key assumptions uncertainties, group_sheet_decision_point",2,,163,232,egould,21/04/2020,egould,21/04/2020
78:8,iterative links between problem formulation phase and define conceptual model,iterative links between problem formulation phase and define conceptual model,case 78,"1-problem_formulation, 2.0-define_conceptual_model, 2.6-define_conceptual_model: iterative link, group_sheet_decision_point",4,,163,240,egould,21/04/2020,egould,21/04/2020
81:8,"Applies to phase description: Largely influenced by school of thinking, what you like, what you know…","Applies to phase description: Largely influenced by school of thinking, what you like, what you know / are used to, what client wants",case 81,"3.1-formalise_specify_model: choose model class, framework and approach, group_sheet_decision_point, PRT-approach",3,"Thus, a question that addresses this, is “justify and explain why you chose this modelling method.",383,516,egould,21/04/2020,egould,21/04/2020
86:8,performance metric needs to be sensitive to the problem,performance metric needs to be sensitive to the problem,case 86,"4.2-model_calibration_fitting_checking: choose performance criteria, group_sheet_decision_point",2,"“Choose Performance Criteria""",305,360,egould,21/04/2020,egould,21/04/2020
87:8,expert has more / different expectations from models than non-expert,expert has more / different expectations from models than non-expert,case 87,"3.7-formalise_specify_model: Specify model assumptions and uncertainties, group_sheet_decision_point",2,Add step to phase to deal with different expectations between expert / non-expert stakeholder / client etc?,305,373,egould,21/04/2020,egould,21/04/2020
88:8,3 levels of hypothesis,3 levels of hypothesis,case 88,"3.6-formalise_specify_model: specify formal model, group_sheet_decision_point",2,,383,405,egould,21/04/2020,egould,21/04/2020
91:8,"""Laugh Test"" Does model fit needs of client / user? Is some form of validation?","""Laugh Test"" Does model fit needs of client / user? Is some form of validation?",case 91,"5.0-model_validation_evaluation, 6.2-model_application: expert / client / stakeholder evaluation, group_sheet_decision_point",3,Not quite certain where this should fall under. Speaks to ‘missing step or phase’ that were identified in the workshop.,387,466,egould,21/04/2020,egould,21/04/2020
92:1,checking model assumptions and assessing potential lack of fit,checking model assumptions and assessing potential lack of fit,Conn-2018-Ecol_Monogr.pdf,4.4-model_calibration_fitting_checking: model checking,1,,1:1234,1:1296,egould,29/04/2020,egould,29/04/2020
92:2,"Alternatively, lesser known approaches to model checking, such as prior predictive checks, cross-val…","Alternatively, lesser known approaches to model checking, such as prior predictive checks, cross-validation probabil- ity integral transforms, and pivot discrepancy measures may produce more accurate characterizations of goodness-of-fit but are not as well known to ecologists",Conn-2018-Ecol_Monogr.pdf,4.4-model_calibration_fitting_checking: model checking,1,,1:1666,1:1942,egould,29/04/2020,egould,29/04/2020
92:3,a suite of visual and targeted diagnostics can be used to examine violations of different model assu…,"a suite of visual and targeted diagnostics can be used to examine violations of different model assumptions and lack of fit at differ- ent levels of the modeling hierarchy, and to check for residual temporal or spatial autocorrelation.",Conn-2018-Ecol_Monogr.pdf,4.4-model_calibration_fitting_checking: model checking,1,,1:1957,1:2192,egould,29/04/2020,egould,29/04/2020
92:4,There are multiple ways of assessing a model’s performance in representing the system being stud- ie…,"There are multiple ways of assessing a model’s performance in representing the system being stud- ied. A first step is often to examine diagnostics that compare observed data to model output to pinpoint if and where any systematic differences occur. This process, which we term model checking, is a critical part of statistical inference because it helps diagnose assumption violations and illumi- nate places where a model might be amended to more faith- fully represent gathered data",Conn-2018-Ecol_Monogr.pdf,4.4-model_calibration_fitting_checking: model checking,1,,1:4658,1:5143,egould,29/04/2020,egould,29/04/2020
92:5,"Following this step, one might proceed to compare the performance of alternative models embodying di…","Following this step, one might proceed to compare the performance of alternative models embodying different hypotheses using any number of model comparison or out-of-sample predictive performance met- rics (see Hooten and Hobbs 2015 for a review) to gauge the support for alternative hypotheses or optimize predictive ability (Fig. 1)",Conn-2018-Ecol_Monogr.pdf,"4.5-model_calibration_fitting_checking: model fitting and calibration, 5.0-model_validation_evaluation",2,Where model checking fits into the broader modelling workflow.,1:5145,1:5479,egould,29/04/2020,egould,29/04/2020
93:1,3.8. Conditional verification including diagnostic checking,3.8. Conditional verification including diagnostic checking,Jakeman-2006-Environmental_Modelling_and_Softw.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,10:3766,10:3825,egould,29/04/2020,egould,29/04/2020
93:2,"Once identified, the model must be ‘conditionally’ verified and tested to ensure it is sufficiently…","Once identified, the model must be ‘conditionally’ verified and tested to ensure it is sufficiently robust, i.e. insensitive to possible but practically insignificant changes in the data and to possible deviations of the data and system from the idealising assumptions made (e.g. of Gaussian distribution of measure- ment errors, or of linearity of a relation within the model). It is also necessary to verify that the interactions and outcomes of the model are feasible and defensible, given the objectives and the prior knowledge. Of course, this eighth step should in- volve as wide a range of quantitative and qualitative criteria as circumstances allow.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,10:3826,10:4484,egould,29/04/2020,egould,29/04/2020
93:3,"Quantitative verification is traditionally attempted, but rarely against a wide range of criteria. C…","Quantitative verification is traditionally attempted, but rarely against a wide range of criteria. Criteria may include goodness of fit (comparison of means and variances of ob- served versus modelled outputs), tests on residuals or errors (for heteroscedasticity, cross-correlation with model variables, autocorrelation, isolated anomalously large values) and, par- ticularly for relatively simple empirical models, the speed and certainty with which the parameter estimates converge as more input-output observations are processed.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,10:4485,10:5018,egould,29/04/2020,egould,29/04/2020
93:4,Qualitative verification preferably involves knowledgeable data suppliers or model users who are not…,"Qualitative verification preferably involves knowledgeable data suppliers or model users who are not modellers. Where the model does not act feasibly or credibly, the assumptions, including structure and data assumptions, must be re-evaluated. Indeed, this stage of model development may involve reassess- ment of the choices made at any previous stage. Checking of a model for feasibility and credibility is given little promi- nence in the literature because it is largely informal and case-specific, but it is plainly essential for confidence in the model’s outputs. Again this is a very important step, not only to check the model’s believability, but to build the client’s confidence in the model. It assumes sufficient time for this checking and enough flexibility of model structure to allow modifications. Often these assumptions are not met.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,10:5019,10:5869,egould,29/04/2020,egould,29/04/2020
93:8,"Details of such an approach are still at an early stage of de- velopment, but should extend to: test…","Details of such an approach are still at an early stage of de- velopment, but should extend to: testing the sensitivity of the model to plausible changes in input parameters; where possible or desirable, changes in assumptions about model structure; as well as documentation and critical scrutiny of the process by which the model has been developed, including the assump- tions invoked. A critical difference from traditional model ‘‘validation’’ is the openly subjective nature of such criteria.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,11:5419,11:5916,egould,29/04/2020,egould,29/04/2020
93:12,"documentation of the nature (identity, provenance, quan- tity and quality) of the data used to drive…","documentation of the nature (identity, provenance, quan- tity and quality) of the data used to drive, identify and test the model;",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,2.4-define_conceptual_model: prior knowledge data specification and evaluation,1,"Jakeman, A. J., Letcher, R. A., & Norton, J. P. (2006). Ten iterative steps in development and evaluation of environmental models. Environmental Modelling & Software, 21(5), 602-614. doi:10.1016/j.envsoft.2006.01.004",12:663,12:793,egould,29/04/2020,egould,29/04/2020
93:13,clear statement of the objectives and clients of the model- ling exercise;,clear statement of the objectives and clients of the model- ling exercise;,Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"1.1-problem_formulation: define model purpose, PRT item",2,,12:585,12:659,egould,29/04/2020,egould,29/04/2020
93:14,justification of the methods and criteria employed in calibration;,justification of the methods and criteria employed in calibration;,Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"4.1-model_calibration_fitting_checking: Choose model calibration and validation scheme, 4.2-model_calibration_fitting_checking: choose performance criteria",2,,12:896,12:962,egould,29/04/2020,egould,29/04/2020
93:16,"Some modelling approaches are able explicitly to articulate uncertainty due to data, measurements or…","Some modelling approaches are able explicitly to articulate uncertainty due to data, measurements or baseline conditions, by providing estimates of uncertainty, usually in probabilistic form such as parameter covariance. Others require comprehen- sive testing of the model to develop this understanding.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,11:1028,11:1331,egould,29/04/2020,egould,29/04/2020
93:17,"The results from extensive sensitivity testing can be diffi- cult to interpret, because of the numbe…","The results from extensive sensitivity testing can be diffi- cult to interpret, because of the number and complexity of cause-effect relations tested. To minimise the difficulty, clear priorities are needed for which features of which variables to examine, and which uncertainties to cover. A good deal of trial and error may be required to fix these priorities",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,11:2215,11:2576,egould,29/04/2020,egould,29/04/2020
93:18,Few approaches explicitly consider uncertainty introduced by the system conceptualisation or model s…,"Few approaches explicitly consider uncertainty introduced by the system conceptualisation or model structure. Alterna- tive structures and conceptualisations are unlikely to be exam- ined after an early stage. The reasons include preferences of the developer, compatibility with previous practice or other bodies’ choices, availability of software tools, agency policy, peer pressure and fashion within technical communities, and shortage of time and resources. It is hard to see how this sort of uncertainty can be taken into account beyond remain- ing alert to any compromises and doubts in such choices.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,11:2578,11:3184,egould,29/04/2020,egould,29/04/2020
93:19,"model equifinality, recognising that there often is a wide range of models capable of yielding simil…"," model equifinality, recognising that there often is a wide range of models capable of yielding similar predictions. Uncertainty in models (Walker et al., 2003) stems from incom- plete system understanding (which processes to include, which processes interact); from imprecise, finite and often sparse data and measurements; and from uncertainty in the baseline inputs and conditions for model runs, including predicted in- puts. In Van der Sluijs et al. (2005) uncertainties are consid- ered from a non-technical standpoint, to include those associated with problem framing, indeterminacies and value- ladenness. Their procedure is important if these attributes dominate. A diagnostic diagram can be used to synthesize re- sults of quantitative parameter sensitivity analysis and qualita- tive review of parameter strength (so-called pedigree analysis). It is a reflective approach where process is as important as technical assessments.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,11:89,11:1027,egould,29/04/2020,egould,29/04/2020
93:20,"Model uncertainty must be considered in the context of the purposes of the model. For example, discr…","Model uncertainty must be considered in the context of the purposes of the model. For example, discrepancies between actual output, model output and observed output may be im- portant for forecasting models, where cost, benefit and risk over a substantial period must be gauged, but much less criti- cal for decision-making or management models where the user may be satisfied with knowing that the predicted ranking order of impacts of alternative scenarios or management options is likely to be correct, with only a rough indication of their sizes.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis, PRT justification",2,,11:3827,11:4377,egould,29/04/2020,egould,29/04/2020
93:21,The next choice is of how to estimate the parameter values and supply non-parametric variables and/o…,"The next choice is of how to estimate the parameter values and supply non-parametric variables and/or data (e.g. distrib- uted boundary conditions). The parameters may be calibrated all together by optimising the fit of the model outputs to ob- served outputs, or piecemeal by direct measurement or infer- ence from secondary data, or both. Coarse parameter values indicating presence or absence of a factor or the rough timing of a seasonal event, for instance, might be found by eliciting expert opinion",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,3.4-Choose approach for identifying model structure and parameters,1,,9:2137,9:2642,egould,29/04/2020,egould,29/04/2020
93:22,The parameter estimation criteria (hardly ever a single crite- rion) reflect the desired properties…,"The parameter estimation criteria (hardly ever a single crite- rion) reflect the desired properties of the estimates. For example we might seek robustness to outliers (bad data), unbiasedness and statistical efficiency, along with acceptable prediction performance on the data set used for calibration.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,3.4-Choose approach for identifying model structure and parameters,1,,9:3667,9:3969,egould,29/04/2020,egould,29/04/2020
93:23,3.6. Choice of estimation performance criteria and technique,3.6. Choice of estimation performance criteria and technique,Jakeman-2006-Environmental_Modelling_and_Softw.pdf,3.4-Choose approach for identifying model structure and parameters,1,,9:3606,9:3666,egould,29/04/2020,egould,29/04/2020
93:24,Choice of how model structure and parameter values are to be found,Choice of how model structure and parameter values are to be found,Jakeman-2006-Environmental_Modelling_and_Softw.pdf,3.3-formalise_specify_model: choose how to find structure and parameters,1,,9:909,9:975,egould,29/04/2020,egould,29/04/2020
93:29,"These questions aren’t asked thoroughly enough at the beginning of model projects. That said, the in…","These questions aren’t asked thoroughly enough at the beginning of model projects. That said, the initial answers can easily change as the project develops, especially when managers are involved, emphasiz- ing again the need for iteration",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"3.1-formalise_specify_model: choose model class, framework and approach, PRT justification",2,,8:4654,8:4892,egould,29/04/2020,egould,29/04/2020
93:32,Definition of the purposes for modelling It is a truism that the reasons for modelling should have a…,"Definition of the purposes for modellingIt is a truism that the reasons for modelling should have a large influence on the selecting of a model family or families (see Section 2.5) to represent the system, and on the nature and level of diagnostic checking and model evaluation. However, it is not necessarily easy to be clear about what the purposes are. Different stakeholders will have different degrees of interest in the possible purposes of a single model. For example, a re- source manager is likely to be most concerned with prediction, while a model developer or scientific user may place higher stress on the ability of the model to show what processes dom- inate behaviour of the system. That said, better understanding is valuable for all parties as part of defining the problem and possible solutions, and as a means of assessing how much trustto place in the model. It is important to recognize that some purposes, particularly increased understanding of the system and data, may be realised well even if the final model is poor in many respects. An inaccurate model may still throw light on how an environmental system works.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"1.1-problem_formulation: define model purpose, PRT justification",2,"Definition of the purposes for modellingIt is a truism that the reasons for modelling should have a large influence on the selecting of a model family or families (see Section 2.5) to represent the system, and on the nature and level of diagnostic checking and model evaluation. However, it is not necessarily easy to be clear about what the purposes are. Different stakeholders will have different degrees of interest in the possible purposes of a single model. For example, a re- source manager is likely to be most concerned with prediction, while a model developer or scientific user may place higher stress on the ability of the model to show what processes dom- inate behaviour of the system. That said, better understanding is valuable for all parties as part of defining the problem and possible solutions, and as a means of assessing how much trustto place in the model. It is important to recognize that some purposes, particularly increased understanding of the system and data, may be realised well even if the final model is poor in many respects. An inaccurate model may still throw light on how an environmental system works. Jakeman, A. J., Letcher, R. A., & Norton, J. P. (2006). Ten iterative steps in development and evaluation of environmental models. Environmental Modelling & Software, 21(5), 602-614. doi:10.1016/j.envsoft.2006.01.004",6:752,6:1894,egould,30/04/2020,egould,30/04/2020
93:33,gaining a better qualitative understanding of the system (by means including social learning by inte…," gaining a better qualitative understanding of the system (by means including social learning by interest groups);􏰀 knowledge elicitation and review;􏰀 data assessment, discovering coverage, limitations, incon-sistencies and gaps;􏰀 concise summarising of data: data reduction;􏰀 providing a focus for discussion of a problem;􏰀 hypothesis generation and testing;􏰀 prediction, both extrapolation from the past and ‘‘what if’’exploration;􏰀 control-system design: monitoring, diagnosis, decision-making and action-taking (in an environmental context,adaptive management);􏰀 short-term forecasting (worth distinguishing from longer-term prediction, as it usually has a much narrower focus); 􏰀 interpolation: estimating variables which cannot be mea-sured directly (state estimation), filling gaps in data;􏰀 providing guidance for management and decision-making",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"1.1-problem_formulation: define model purpose, PRT example, PRT justification",3,,6:1915,6:2792,egould,30/04/2020,egould,30/04/2020
93:34,"These motives are not mutually exclusive, of course, but the modeller has to establish the purposes…","These motives are not mutually exclusive, of course, but the modeller has to establish the purposes and priorities within the",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"1.1-problem_formulation: define model purpose, PRT justification",2,"""list, because of their influence on the choices to be made at later stages.”Jakeman, A. J., Letcher, R. A., & Norton, J. P. (2006). Ten iterative steps in development and evaluation of environmental models. Environmental Modelling & Software, 21(5), 602-614. doi:10.1016/j.envsoft.2006.01.004",6:2794,6:2919,egould,30/04/2020,egould,30/04/2020
93:35,"list, because of their influence on the choices to be made at later stages. For example, economy in…","list, because of their influence on the choices to be made at later stages. For example, economy in the degrees of freedom of a prediction model (‘‘parsimony’’) is important if the model is to register the consistent behaviour observed in the data but not the ephemeral, inconsistent ‘‘noise.’’ Experience confirms that it is often counterproductive to include much detail in a prediction model for a restricted purpose (Jakeman and Hornberger, 1993). Conversely, a model designed to increase insight into the processes which determine the system’s overall behaviour has to be complex enough to mimic those processes, even if only very approximately",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"1.1-problem_formulation: define model purpose, PRT justification",2,"Jakeman, A. J., Letcher, R. A., & Norton, J. P. (2006). Ten iterative steps in development and evaluation of environmental models. Environmental Modelling & Software, 21(5), 602-614. doi:10.1016/j.envsoft.2006.01.004",7:79,7:728,egould,30/04/2020,egould,30/04/2020
93:37,Specification of the modelling context: scope and resources,Specification of the modelling context: scope and resources,Jakeman-2006-Environmental_Modelling_and_Softw.pdf,1.2-problem_formulation: specify modelling context,1,"Jakeman, A. J., Letcher, R. A., & Norton, J. P. (2006). Ten iterative steps in development and evaluation of environmental models. Environmental Modelling & Software, 21(5), 602-614. doi:10.1016/j.envsoft.2006.01.004",7:1897,7:1956,egould,30/04/2020,egould,30/04/2020
93:38,This second step identifies: 􏰀 the specific questions and issues that the model is to address; 􏰀 t…,"This second step identifies:􏰀 the specific questions and issues that the model is to address;􏰀 the interest groups, including the clients or end-users of the model;􏰀 the outputs required;􏰀 the forcing variables (drivers);􏰀 the accuracy expected or hoped for;􏰀 temporal and spatial scope, scale and resolution (but seealso Section 3.3);􏰀 the time frame to complete the model as fixed, for exam-ple, by when it must be ready to help a decision;􏰀 the effort and resources available for modelling and oper-ating the model, and;􏰀 flexibility; for example, can the model be quickly recon-figured to explore a new scenario proposed by a manage- ment group?",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,1.2-problem_formulation: specify modelling context,1,,7:1957,7:2628,egould,30/04/2020,egould,30/04/2020
93:39,"A crucial step here is to decide the extent of the model, i.e. where the boundary of the modelled sy…","A crucial step here is to decide the extent of the model, i.e. where the boundary of the modelled system is. Everything out- side and not crossing the boundary is ignored. Everything crossing the boundary is treated as external forcing (knownor unknown) or as outputs (observed or not). The choice of a boundary is closely tied in with the choice of how far to ag- gregate the behaviour inside it. Classical thermodynamics gives an object lesson in the benefits of choosing the boundary and degree of aggregation well, so as to discover simple rela- tions between a small number of aggregated variables (e.g. en- ergy) crossing the boundary, without having to describe processes inside the boundary in detail",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,1.2-problem_formulation: specify modelling context,1,"Jakeman, A. J., Letcher, R. A., & Norton, J. P. (2006). Ten iterative steps in development and evaluation of environmental models. Environmental Modelling & Software, 21(5), 602-614. doi:10.1016/j.envsoft.2006.01.004",7:2629,7:3338,egould,30/04/2020,egould,30/04/2020
93:40,"In environmental man- agement, deciding on the boundary and degree of aggregation is a critical but…","In environmental man- agement, deciding on the boundary and degree of aggregation is a critical but very difficult step. It can usually only be learnt through trial and error, since managers and stakeholders usu- ally do not initially know the boundaries of what should be modelled.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"1.2-problem_formulation: specify modelling context, PRT justification",2,,7:3340,7:3622,egould,30/04/2020,egould,30/04/2020
93:41,Conceptualisation refers to basic premises about the work- ing of the system being modelled. It migh…,"Conceptualisation refers to basic premises about the work- ing of the system being modelled. It might employ aids to thinking such as an influence diagram, linguistic model, block diagram or bond graph (Gawthrop and Smith, 1996; Well- stead, 1979), showing how model drivers are linked to internal (state) variables and outputs (observed responses)",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,2.1-define_conceptual_model: choose elicitation and representation method,1,,7:4217,7:4565,egould,30/04/2020,egould,30/04/2020
93:44,"This third step defines the data, prior knowledge and as- sumptions about processes. The procedure i…","This third step defines the data, prior knowledge and as- sumptions about processes. The procedure is mainly qualita- tive to start with, asking what is known of the processes, what records, instrumentation and monitoring are available, and how far they are compatible with the physical and tempo- ral scope dictated by the purposes and objectives. However, it becomes quantitative as soon as we have to decide what to in- clude and what can be simplified or neglected. What variables are to be included, in how much detail? Once the outputs are selected, a rough assessment is needed of which drivers they are sensitive to and what internal processes influence the rela- tions between the drivers and outputs; this will usually be partly a quantitative assessment.",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, PRT justification",2,,7:5165,7:5930,egould,30/04/2020,egould,30/04/2020
93:45,"chosen but, as for all these decisions, the choices may have to be revised as experience grow","chosen but, as for all these decisions, the choices may have to be revised as experience grow",Jakeman-2006-Environmental_Modelling_and_Softw.pdf,"2.6-define_conceptual_model: iterative link, modeling as way of increasing system knowledge, PRT-approach",3,,8:0,8:93,egould,30/04/2020,egould,30/04/2020
94:1,"model analysis’, examining the model’s sensitivity to changes in param- eters and formulation to und…","model analysis’, examining the model’s sensitivity to changes in param- eters and formulation to understand the model’s main behaviours and describing and justifying simulation experiments;",Grimm_et_al_2014.pdf,5.2-model_validation_evaluation: model analysis,1,,5:4781,5:4970,egould,29/04/2020,egould,29/04/2020
94:2,"‘model output corroboration’, comparing model output to data and patterns that were not used for mod…","‘model output corroboration’, comparing model output to data and patterns that were not used for model development and parame- terization",Grimm_et_al_2014.pdf,5.3-model_validation_evaluation: model output corroboration,1,,5:4980,5:5117,egould,29/04/2020,egould,29/04/2020
94:3,"(iv) ‘model output verification’, comparing model output to the data and patterns that guided model…","(iv) ‘model output verification’, comparing model output to the data and patterns that guided model design and calibration;",Grimm_et_al_2014.pdf,4.4-model_calibration_fitting_checking: model checking,1,,5:4652,5:4775,egould,29/04/2020,egould,29/04/2020
94:4,"(iii) ‘implementation verification’, checking the model’s implementation in equations and software"," (iii) ‘implementation verification’, checking the model’s implementation in equations and software",Grimm_et_al_2014.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,5:4551,5:4650,egould,29/04/2020,egould,29/04/2020
94:5,"(ii) ‘conceptual model evaluation’, scrutinizing the simplifying assumptions under- lying a model’s…","(ii) ‘conceptual model evaluation’, scrutinizing the simplifying assumptions under- lying a model’s design",Grimm_et_al_2014.pdf,2.5-define_conceptual_model: conceptual model evaluation,1,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TR",5:4444,5:4550,egould,29/04/2020,egould,29/04/2020
94:6,"(i) ‘data evaluation’, assessing the quality of numerical and qualita- tive data used for model deve…","(i) ‘data evaluation’, assessing the quality of numerical and qualita- tive data used for model development and testing",Grimm_et_al_2014.pdf,2.4-define_conceptual_model: prior knowledge data specification and evaluation,1,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TR",5:4323,5:4442,egould,29/04/2020,egould,29/04/2020
94:8,Conceptual model evaluation. This element is defined by Augusiak et al. (2014) as “the critical asse…,"Conceptual model evaluation. This element is defined by Augusiak et al. (2014) as “the critical assessment of the simplifying assumptions underlying a model’s design”. The design of any mathematical or simulation model is based on a concep- tual model which reflects our preliminary understanding and perception of the system to be represented in the model. For example, we may focus on nutrients and energy, speciescomposition, or individual organisms",Grimm_et_al_2014.pdf,"2.5-define_conceptual_model: conceptual model evaluation, PRT justification",2,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TR",8:2339,8:2792,egould,29/04/2020,egould,29/04/2020
94:10,"The evaluation applies to the overall model structure and sometimes to submodels, for example of met…","The evaluation applies to the overall model structure and sometimes to submodels, for example of metabolism, competition among individuals, movement, or the physical environment.",Grimm_et_al_2014.pdf,2.5-define_conceptual_model: conceptual model evaluation,1,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TR",8:2909,8:3087,egould,29/04/2020,egould,29/04/2020
94:11,"In detail, this evaluation lists and explains the most important conceptual design decisions: spatia…","In detail, this evaluation lists and explains the most important conceptual design decisions: spatial and temporal scales, selec- tion of entities and processes, representation of stochasticity and heterogeneity, consideration of local versus global interac- tions, environmental drivers, etc",Grimm_et_al_2014.pdf,2.2:define_conceptual_model: specify key assumptions uncertainties,1,,8:3088,8:3380,egould,29/04/2020,egould,29/04/2020
94:13,Implementation verification. This term is defined by Augusiak et al. (2014) as “the critical assessm…,Implementation verification. This term is defined by Augusiak et al. (2014) as “the critical assessment of (1) whether the com- puter code for implementing the model has been thoroughly tested for programming errors and (2) whether the implemented model performs as indicated by the model description”. ,Grimm_et_al_2014.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,8:4142,8:4445,egould,29/04/2020,egould,29/04/2020
94:14,"For instance, implementation verification might be conducted by peer-reviewing the code, i.e., other…","For instance, implementation verification might be conducted by peer-reviewing the code, i.e., other scientists thoroughly com- paring it with the written formulation of the model, or by independently implementing submodels. This TRACE element provides evidence that the model software has been thoroughly tested and accurately implements the model description.",Grimm_et_al_2014.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,8:4445,8:4806,egould,29/04/2020,egould,29/04/2020
94:15,Model output verification. Augusiak et al. (2014) define this ele- ment as “the critical assessment…,Model output verification. Augusiak et al. (2014) define this ele-ment as “the critical assessment of (1) how well model output matches observations and (2) how much calibration and effects of environmental drivers were involved in obtaining good fits of model output and data”,Grimm_et_al_2014.pdf,5.1-model_validation_evaluation: model output verification,1,,9:101,9:379,egould,29/04/2020,egould,29/04/2020
94:16,"In developing any model, we try to make it reproduce some features or patterns of the real system be…","In developing any model, we try to make it reproduce some features or patterns of the real system before claiming that it is a good enough representation.",Grimm_et_al_2014.pdf,5.1-model_validation_evaluation: model output verification,1,,9:381,9:535,egould,29/04/2020,egould,29/04/2020
94:17,we list the features we used plus the quantitative crite- ria for deciding whether a certain observa…,we list the features we used plus the quantitative crite- ria for deciding whether a certain observation was matched by the model. ,Grimm_et_al_2014.pdf,5.1-model_validation_evaluation: model output verification,1,,9:559,9:690,egould,29/04/2020,egould,29/04/2020
94:18,Output verification involves what often is referred to as ‘face validation’ and more formal tests. F…,"Output verification involves what often is referred to as ‘face validation’ and more formal tests. Face validation can be defined as: “All methods that rely on natural human intelligence” (Klügl, 2008, p. 39). Examples listed by Klügl (2008) include: “Structured walk-throughs, expert assessments of descriptions, animations of results”. Klügl (2008) accordingly concludes that face validity shows that a model’s processes and outcomes are reasonable and plausible within its theoretical basis and the knowledge of system experts or stakeholders. It should be noted, however, that system experts and stakeholders may disagree on the type of data and knowledge they have. Therefore more formal tests are required that are based on multiple quantitative criteria for a model matching data (e.g., Railsback and Grimm, 2012, Chapter 20.4.2).",Grimm_et_al_2014.pdf,5.1-model_validation_evaluation: model output verification,1,,9:1114,9:1954,egould,29/04/2020,egould,29/04/2020
94:19,Evaluation of output verification needs to consider such concerns as over-fitting and extrapolation.…,"Evaluation of output verification needs to consider such concerns as over-fitting and extrapolation. The higher the pro- portion of calibrated, guesstimated, or uncertain parameters (see TRACE element ‘Model analysis’ below), the higher the risk that the model seems to work correctly (e.g., because it fits calibration data well) but for the wrong reasons, i.e., has not captured the mechanisms of the real system.",Grimm_et_al_2014.pdf,4.3-model_calibration_fitting_checking: Implementation verification,1,,9:1955,9:2370,egould,29/04/2020,egould,29/04/2020
94:20,"Model developers naturally often claim that their models are realistic enough for their purpose, but…","Model developers naturally often claim that their models are realistic enough for their purpose, but in this TRACE element they should summarize why they believe so, with suppor- ting evidence. This information enables users to scrutinize the modeller’s claim and to critically assess how well model output matches observations, the degree to which the match results from calibration and environmental drivers, and how much themodel’s reliability is limited by use of empirical parameters thatreflect only a narrow range of conditions.",Grimm_et_al_2014.pdf,"5.1-model_validation_evaluation: model output verification, PRT justification",2,,9:3597,9:4134,egould,29/04/2020,egould,29/04/2020
94:21,Model analysis. This element is defined by Augusiak et al. (2014) as “the assessment of (1) how sens…,"Model analysis. This element is defined by Augusiak et al. (2014)as “the assessment of (1) how sensitive model output is to changes in model parameters (sensitivity analysis), and (2) how well the emergence of model output has been understood”",Grimm_et_al_2014.pdf,"5.2-model_validation_evaluation: model analysis, 5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis",2,,9:4138,9:4382,egould,29/04/2020,egould,29/04/2020
94:22,The purpose of the element is to prevent blind trust in the model output by asking “How did this out…,"The purpose of the element is to prevent blind trust in the model output by asking “How did this output emerge?”, and to chal- lenge the model, which might look impressive, by asking “Does verification still look good if I change one or more parameters a bit?”",Grimm_et_al_2014.pdf,"5.2-model_validation_evaluation: model analysis, PRT justification",2,,9:4384,9:4644,egould,29/04/2020,egould,29/04/2020
94:23,"Thus, foremost here we document how we made sure that we understood a model’s main mechanisms. For e…","Thus, foremost here we document how we made sure that we understood a model’s main mechanisms. For example, if recov- ery after disturbance is strongly affected by a certain parameter and, thus, the processes the parameter represents, we should be able to explain why this parameter was so important. We can learn much about a model by performing controlled simulation experiments: keeping most parameters constant and varying one or a few over a wider range, and exploring the effect on one or more output variables. Simulation experiments should also include simplified model versions, in which the environ- ment is made more homogenous and constant, system size is reduced, and certain processes are deactivated. Initial conditions and input data are other model components to which sensitivity should be analyzed.",Grimm_et_al_2014.pdf,5.2-model_validation_evaluation: model analysis,1,,9:4645,9:5462,egould,29/04/2020,egould,29/04/2020
94:24,"TRACE should not include details on all these experiments, but give an overview of what kind of expe…","TRACE should not include details on all these experiments, but give an overview of what kind of experiments were per- formed and present results from experiments that significantly increased understanding but could not be included in the paper or report.",Grimm_et_al_2014.pdf,5.2-model_validation_evaluation: model analysis,1,,9:5463,9:5717,egould,29/04/2020,egould,29/04/2020
94:25,Local sensitivity analysis is important for developing a first understanding of a model by evaluatin…,"Local sensitivity analysis is important for developing a first understanding of a model by evaluating how sensitive output is to small changes in one parameter at a time. The analysis can produce conclusions about model uncertainty: if the parameters to which the model is most sensitive are the most uncertain ones, the entire model will be quite uncertain. Moreover, such param- eters indicate which processes are most important for certain model outputs",Grimm_et_al_2014.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,9:5718,9:6174,egould,29/04/2020,egould,29/04/2020
94:26,Uncertainty analysis can augment sensitivity analysis by demonstrating how uncertainty in model para…,Uncertainty analysis can augment sensitivity analysis by demonstrating how uncertainty in model parameters translates into uncertainty in model output.,Grimm_et_al_2014.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,,9:6725,9:6876,egould,29/04/2020,egould,29/04/2020
94:27,Parameters often represent entire processes that the modeller chose not to represent explicitly. Sub…,"Parameters often represent entire processes that the modeller chose not to represent explicitly. Submodels represent processes that are represented explicitly in more detail; therefore, sensi- tivity analysis should also be applied to important submodels by contrasting alternative submodels",Grimm_et_al_2014.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,"At what level (parameter, submodel, global etc) will you perform the analyses. Describe the analysis for each level.",9:6877,9:7168,egould,29/04/2020,egould,29/04/2020
94:28,"For example, a submodel describing movement might be based on complex decision mak- ing, but contras…","For example, a submodel describing movement might be based on complex decision mak- ing, but contrasting this submodel with simpler, or even more complex, alternatives can provide insights into how important or useful it was to choose this very model design. This sensi- tivity analysis of submodels corresponds to what Railsback and Grimm (2012) refer to as ‘pattern-oriented theory development’: which submodel best causes the full model to reproduce a set of observed patterns?",Grimm_et_al_2014.pdf,5.7-model_validation_evaluation: sensitivity analysis and uncertainty analysis,1,Links to what CH described about different components or part of a model.,9:7170,9:7650,egould,29/04/2020,egould,29/04/2020
94:29,"Model users learn from this TRACE element how the model works, i.e., which processes and process int…","Model users learn from this TRACE element how the model works, i.e., which processes and process interactions are impor- tant and explain major behaviours of the model system.",Grimm_et_al_2014.pdf,"5.2-model_validation_evaluation: model analysis, PRT justification",2,,9:7651,9:7826,egould,29/04/2020,egould,29/04/2020
94:30,Model output corroboration. This term is defined by Augusiak et al. (2014) as the “comparison of mod…,"Model output corroboration. This term is defined by Augusiak et al. (2014) as the “comparison of model predictions with inde- pendent data and patterns that were not used, and preferably not even known, while the model was developed, parameter- ized, and verified”",Grimm_et_al_2014.pdf,5.3-model_validation_evaluation: model output corroboration,1,,10:181,10:445,egould,29/04/2020,egould,29/04/2020
94:31,"Most scientists, in particular non-modelers, require this analysis, calling it ‘validation’: for a m…","Most scientists, in particular non-modelers, require this analysis, calling it ‘validation’: for a model to be trusted it should make predictions that are subsequently con- firmed in empirical experiments. Indeed, we consider this the ‘gold standard’ for demonstrating that a model has captured the internal organization of a system sufficiently well. Corroborationis discussed in more depth by Augusiak et al. (2014).",Grimm_et_al_2014.pdf,5.3-model_validation_evaluation: model output corroboration,1,,10:447,10:866,egould,29/04/2020,egould,29/04/2020
94:32,"Model output verification always includes ‘tweaking’, i.e., we try to make a model reproduce certain…","Model output verification always includes ‘tweaking’, i.e., we try to make a model reproduce certain observations by tuning parameters, environmental settings, and submodel formulation. Such adjustments are often necessary to compensate for pro- cesses not included in a model (due to insufficient information or to keep the model simple) but were important in the real sys- tem when the verification data were collected. Making a model simultaneously reproduce multiple observed patterns reduces the risk that the model is completely unrealistic, but does not eliminate this risk. Only when a model predicts phenomena that we even did not think about during model development and test- ing do we have the strongest indicator of its structural realism,because no tweaking could have been involved.",Grimm_et_al_2014.pdf,"5.1-model_validation_evaluation: model output verification, PRT justification",2,,10:867,10:1665,egould,29/04/2020,egould,29/04/2020
94:33,"However, achieving this standard is rarely possible with ecological systems because the empirical ex…","However, achieving this standard is rarely possible withecological systems because the empirical experiments are infeasible: we often build models to address questions such as response to climate change exactly because empirical experi- ments are impossible. Instead, we can directly test independent predictions of submodels. At the system level, we can identify characteristic patterns in model output that are robust and seem characteristic. Then, we can consult the literature or experts to find out how accurate these independent predictions are.",Grimm_et_al_2014.pdf,5.1-model_validation_evaluation: model output verification,1,Qualitative or empirical tests that we can implement in place of being able to test on independent data.,10:1666,10:2218,egould,29/04/2020,egould,29/04/2020
94:34,"Documenting model output corroboration provides model users evidence, in addition to model output ve…","Documenting model output corroboration provides model users evidence, in addition to model output verification, indi- cating the extent to which the model is structurally realistic so that its predictions can be trusted. The model’s purpose should be a primary consideration in determining what model results need corroboration and how quantitatively and closely model results need to reproduce observations. If no corroboration was possible, the modeller should discuss here why, and why and to what degree the model still can be trusted.",Grimm_et_al_2014.pdf,"5.3-model_validation_evaluation: model output corroboration, PRT justification",2,,10:2219,10:2758,egould,29/04/2020,egould,29/04/2020
94:35,Concise text plus tables should summarize what data and knowledge went into the design and parameter…,"Concise text plus tables should summarize what data and knowledge went into the design and parameterization of the model, including references, data sources, and information about where and when those data were collected, and by whom.",Grimm_et_al_2014.pdf,2.4-define_conceptual_model: prior knowledge data specification and evaluation,1,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TRACe",8:1095,8:1329,egould,29/04/2020,egould,29/04/2020
94:36,"f pos- sible, the reliability of the data used should be discussed, as data quality and ecological s…","f pos- sible, the reliability of the data used should be discussed, as data quality and ecological significance might be limited by measure- ment errors, inappropriate experimental design (e.g., number of replicates), and, in particular, the heterogeneity and variabil- ity inherent to environmental systems (Gass, 1983; Wang and Luttik, 2012). Likewise, expert knowledge and the detection of patterns are prone to bias and therefore must be treated with particular caution.",Grimm_et_al_2014.pdf,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, PRT justification",2,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TR",8:1331,8:1805,egould,29/04/2020,egould,29/04/2020
94:37,The document should indicate which parame- ter values were used directly without calibration and whi…,"The document should indicate which parame- ter values were used directly without calibration and which were determined inversely; the methods used for inverse parameter- ization will be described in the TRACE element, “Model output verification”.",Grimm_et_al_2014.pdf,3.4-Choose approach for identifying model structure and parameters,1,,8:1806,8:2052,egould,29/04/2020,egould,29/04/2020
94:38,The data description and evaluation allows model users to (1) see whether a model was mainly built o…,"The data description and evaluation allows model users to (1) see whether a model was mainly built on its authors’ own data and knowledge, or on that of a certain expert or group of experts, or on a systematic evaluation of the literature, and (2) assess how uncertain the data are.",Grimm_et_al_2014.pdf,"2.4-define_conceptual_model: prior knowledge data specification and evaluation, PRT justification",2,"Grimm, V., Augusiak, J., Focks, A., Frank, B. M., Gabsi, F., Johnston, A. S. A., . . . Railsback, S. F. (2014). Towards better modelling and decision support: Documenting model development, testing, and analysis using TR",8:2053,8:2335,egould,29/04/2020,egould,29/04/2020
94:40,"Suitable habitat for soil organisms may be scarce, thus leading to locally high population densities…","Suitable habitat for soil organisms may be scarce, thus leading to locally high population densities, because soil, being more static than water or air, is heterogeneous: physical conditions often vary widely on a scale of a few centimetres. Moreover, toxic chemicals are likely to be unevenly distributed in the soil as well. The spatially explicit individual-based model presented in Meli et al. (2013) is developed to explore the consequences of these heterogeneities for the population dynamics of soil invertebrates, in particular the collembolan Folsomia candida. F. candida is a common arthropod that occurs in soils worldwide and is used as a standard test organism for estimating the effects of pesticides on non-target soil arthropods.",Grimm_et_al_2014.pdf,"1.1-problem_formulation: define model purpose, PRT example",2,,6:5636,6:6381,egould,1/05/2020,egould,1/05/2020
94:41,In order to ensure that the computer code implementing the model works according to its specificatio…,"In order to ensure that the computer code implementing the model works according to its specification in the ODD model description, a series of tests has been performed. These tests included syntax checking of the code, visual testing through NetLogo interface, print statements, spot tests with agent monitors, stress tests with extreme parameters values, test procedures and test programs, and code reviews.",Grimm_et_al_2014.pdf,"4.3-model_calibration_fitting_checking: Implementation verification, PRT example",2,,6:7035,6:7444,egould,1/05/2020,egould,1/05/2020
95:1,Evaluation of model assumptions,Evaluation of model assumptions,Araujo_et_al_2019.pdf,5.5-model_validation_evaluation: evaluate model assumptions,1,,4:6235,4:6266,egould,30/04/2020,egould,30/04/2020
95:2,Evaluation of model outputs,Evaluation of model outputs,Araujo_et_al_2019.pdf,5.6-model_validation_evaluation: evaluate model outputs,1,,4:6310,4:6337,egould,30/04/2020,egould,30/04/2020
95:3,"When evaluating biodiversity models, three critical questions need to be addressed: How robust is th…","When evaluating biodiversity models, three critical questions need to be addressed: How robust is the model to departures from the assumptions? How meaningful are the evaluation metrics used? How predictive is the model when tested against independent data? In addressing these questions, at least three properties of the models can be evaluated (37, 61): realism, accuracy, and generality. Realism is the ability of a model to identify the critical predictors directly affecting the system and to characterize their effects and interac- tions appropriately. Accuracy is the ability of the model to predict events correctly within the system being modeled (e.g., species dis- tributions in the same space and time as the input data). Generality is the ability of the model to predict events outside the modeled system via projection or transfer to a different resolution, geograph- ic location, or time period. Depending on the question at hand, one of these properties might be more important than the other (25)",Araujo_et_al_2019.pdf,"4.2-model_calibration_fitting_checking: choose performance criteria, 5.0-model_validation_evaluation",2,,4:4598,4:5611,egould,30/04/2020,egould,30/04/2020
96:1,Explain analytical objectives,Explain analytical objectives ,Moalemmii-et-al-2019.pdf,1.3.1-problem_formulation: explain analytical objectives,1,"Moallemi, E. A., Elsawah, S., & Ryan, M. J. (2019). Strengthening ‘good’ modelling practices in robust decision support: A reporting guideline for combining multiple model-based methods. Mathematics and Computers in Simu",8:605,8:635,egould,30/04/2020,egould,30/04/2020
96:2,Quotation 96:2,,Moalemmii-et-al-2019.pdf,1.4-problem_formulation: define candidate actions decisions,1,,8:377:515,8:339:135,egould,30/04/2020,egould,30/04/2020
96:3,"Identify outcomes to evaluate decisions under scenarios Descriptive statistic measures, e.g., a mode…","Identify outcomes to evaluate decisions under scenariosDescriptive statistic measures, e.g., a model outcome in the top 10th percentile of worst conditions [73]; satisficing measures, e.g., a minimum performance threshold for achieving a performance objective [65]; regret measures, e.g., choosing a decision with the minimum regret [45] (See more in [50]).",Moalemmii-et-al-2019.pdf,1.3.1-problem_formulation: explain analytical objectives,1,"Descriptive statistic measures, e.g., a model outcome in the top 10th percentile of worst conditions [73]; satisficing measures, e.g., a minimum performance threshold for achieving a performance objective [65]; regret measures, e.g., choosing a decision with the minimum regret [45] (See more in [50]).",8:2578,8:2936,egould,30/04/2020,egould,30/04/2020
96:4,Quotation 96:4,,Moalemmii-et-al-2019.pdf,1.5-problem_formulation: specify scenarios,1,,8:334:514,8:295:136,egould,30/04/2020,egould,30/04/2020
96:5,Quotation 96:5,,Moalemmii-et-al-2019.pdf,2.2:define_conceptual_model: specify key assumptions uncertainties,1,,8:289:512,8:228:134,egould,30/04/2020,egould,30/04/2020
96:6,"Depending on the case, this can be: e.g., to evaluate/trade-off between candidate decisions and anal…","Depending on the case, this can be: e.g., to evaluate/trade-off between candidate decisions and analyse their sensitivity under future scenarios (e.g., [43]); to stress test decisions to find their vulnerabilities in the future (e.g., [10]); to identify worst possible outcomes that a set of decisions could yield (e.g., [21]); to search for ‘good’ decisions which make trade-offs between multiple objectives (e.g., [31]); to search for ‘robust’ decisions making trade-offs between multiple objectives while remaining valid under uncertainty (e.g., [37,53,54]); etc. (also see [36]).",Moalemmii-et-al-2019.pdf,"1.3.1-problem_formulation: explain analytical objectives, PRT example",2,"Moallemi, E. A., Elsawah, S., & Ryan, M. J. (2019). Strengthening ‘good’ modelling practices in robust decision support: A reporting guideline for combining multiple model-based methods. Mathematics and Computers in Simu",8:650,8:1233,egould,1/05/2020,egould,1/05/2020
96:7,Quotation 96:7,,Moalemmii-et-al-2019.pdf,2.2:define_conceptual_model: specify key assumptions uncertainties,1,,13:273:404,13:171:205,egould,5/05/2020,egould,5/05/2020
96:8,Quotation 96:8,,Moalemmii-et-al-2019.pdf,2.2:define_conceptual_model: specify key assumptions uncertainties,1,,13:215:187,13:170:117,egould,5/05/2020,egould,5/05/2020
98:1,"Ensemble: single model, mul􏰀model average .","Ensemble: single model, mul􏰀model average .",Yates-2018-model-transferrabilitee-TREE.pdf,3.4-Choose approach for identifying model structure and parameters,1,,8:1101,8:1145,egould,30/04/2020,egould,30/04/2020
98:2,Scheme: internal cross-valida􏰀on versus external tes􏰀ng on independent datasets ...,Scheme: internal cross-valida􏰀on versus externaltes􏰀ng on independent datasets ...,Yates-2018-model-transferrabilitee-TREE.pdf,4.1-model_calibration_fitting_checking: Choose model calibration and validation scheme,1,,8:1189,8:1274,egould,30/04/2020,egould,30/04/2020
98:3,"Performance: correla􏰀on score, coefficient of determina􏰀on, specificity, sensi􏰀vity, AUC ...","Performance: correla􏰀on score, coefficient of determina􏰀on, specificity, sensi􏰀vity, AUC ...",Yates-2018-model-transferrabilitee-TREE.pdf,3.5-formalise_specify_model: choose estimation performance criteria and technique,1,,8:1275,8:1370,egould,30/04/2020,egould,30/04/2020
98:5,"Predic􏰀on: project model across space, 􏰀me, taxa (into novel condi􏰀ons) ...","Predic􏰀on: project model across space, 􏰀me, taxa (into novel condi􏰀ons) ...",Yates-2018-model-transferrabilitee-TREE.pdf,6.3-model_application: Scenario analysis and simulation,1,,8:1397,8:1475,egould,30/04/2020,egould,30/04/2020
98:6,Visualiza􏰀on: map (or plot) model outputs and associated measure(s) of confidence ...,Visualiza􏰀on: map (or plot) model outputs and associated measure(s) of confidence ...,Yates-2018-model-transferrabilitee-TREE.pdf,6.3-model_application: Scenario analysis and simulation,1,,8:1476,8:1562,egould,30/04/2020,egould,30/04/2020
98:8,Model choice and implementa􏰀on,Model choice and implementa􏰀on,Yates-2018-model-transferrabilitee-TREE.pdf,"3.1-formalise_specify_model: choose model class, framework and approach",1,,8:977,8:1008,egould,30/04/2020,egould,30/04/2020
98:9,"Class: correla􏰀ve, mechanis􏰀c, hybrid ... Algorithm: GLM/GAM, MaxEnt, CLIMEX, FATE-HD ... Ensemble…","Class: correla􏰀ve, mechanis􏰀c, hybrid ...Algorithm: GLM/GAM, MaxEnt, CLIMEX, FATE-HD ... Ensemble: single model, mul􏰀model average ...",Yates-2018-model-transferrabilitee-TREE.pdf,"3.1-formalise_specify_model: choose model class, framework and approach",1,,8:1009,8:1147,egould,30/04/2020,egould,30/04/2020
98:10,"Explora􏰀on: collinearity, spa􏰀al & temporal coverage, quality and resolu􏰀on, outliers, transforma…","Explora􏰀on: collinearity, spa􏰀al & temporal coverage, quality and resolu􏰀on, outliers, transforma􏰀ons ...",Yates-2018-model-transferrabilitee-TREE.pdf,"2.3-define_conceptual_model: identify predictor response variables, data_exploration",2,,8:859,8:968,egould,30/04/2020,egould,30/04/2020
98:11,"Preparati􏰀on: manipula􏰀tion of environmental layers, e.g., standardisa􏰀on, geographic projec􏰀on ...","Prepara􏰀on: manipula􏰀on of environmental layers, e.g., standardisa􏰀on, geographic projec􏰀on ...",Yates-2018-model-transferrabilitee-TREE.pdf,2.4-define_conceptual_model: prior knowledge data specification and evaluation,1,,8:759,8:858,egould,30/04/2020,egould,7/05/2020
98:12,"Response variable(s) Type: presence, absence, abundance ... Species traits: dispersal, physiological…","Response variable(s)Type: presence, absence, abundance ...Species traits: dispersal, physiological tolerance ...",Yates-2018-model-transferrabilitee-TREE.pdf,2.3-define_conceptual_model: identify predictor response variables,1,,8:614,8:728,egould,30/04/2020,egould,30/04/2020
98:13,"Focal taxa and study objec􏰀ves Organisms: terrestrial, marine, freshwater ... Common goals: conserv…","Focal taxa and study objec􏰀vesOrganisms: terrestrial, marine, freshwater ... Common goals: conserva􏰀on planning, impact assessment, theore􏰀cal ecology, niche evolu􏰀on ...",Yates-2018-model-transferrabilitee-TREE.pdf,"1.1-problem_formulation: define model purpose, PRT item",2,"Yates, K. L., Bouchet, P. J., Caley, M. J., Mengersen, K., Randin, C. F., Parnell, S., . . . Sequeira, A. M. M. (2018). Outstanding Challenges in the Transferability of Ecological Models. Trends Ecol. Evol. (Amst.), 33(1",8:430,8:605,egould,30/04/2020,egould,30/04/2020
100:1,An ongoing challenge with mental models research is that it is difficult to decide which method is t…,"An ongoing challenge with mental models research is that it is difficult to decide which method is the most appropri- ate for a given context and why. Several methods are avail- able to elicit mental models, namely interviews (Morgan, Fischoff, Bostrom, & Atman, 2002), drawings (Jones et al., 2014), repertory grids (Kelly, 1955), and a variety of map- ping techniques including influence diagrams (Diffenbach, 1982), cognitive maps (Axelrod 1976), fuzzy cognitive maps (Özesmi & Özesmi, 2004), and Bayesian belief networks (BBN) (e.g., Pollino, Woodberry, Nicholson, Korb, & Hart, 2007). Depending on the application, some elicitation and analysis methods will be more suitable than others, yet the literature provides little guidance in how to navigate among the choices (see Jones et al., 2014).",Moon_et_al_2019.pdf,2.1-define_conceptual_model: choose elicitation and representation method,1, Moon et al. 2019 provide guidance on how to choose different methods of eliciting mental models. ,2:1334,2:2135,egould,30/04/2020,egould,30/04/2020
101:1,"The acquisition, preparation and selection of predictor variables are thus crucial steps for constru…","The acquisition, preparation and selection of predictor variables are thus crucial steps for construction of species distributions models.",Araujo_et_al_2019 781.pdf,2.3-define_conceptual_model: identify predictor response variables,1,,7:469,7:607,egould,30/04/2020,egould,30/04/2020
101:2,The gold standard requires that the conditions on which the response variable depends be captured by…,"The gold standard requires that the conditions on which the response variable depends be captured by the predictor variables selected, at the spatial and temporal resolutions chosen, and that the uncertainty around these conditions (i.e., based on measurement and/or model errors) be quantifiable in the final model.",Araujo_et_al_2019 781.pdf,2.3-define_conceptual_model: identify predictor response variables,1,,7:608,7:924,egould,30/04/2020,egould,30/04/2020
